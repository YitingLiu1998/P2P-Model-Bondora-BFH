{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79d5e091",
   "metadata": {},
   "source": [
    "# Chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8ed997",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T09:55:09.701230Z",
     "start_time": "2023-05-19T09:55:09.682282Z"
    }
   },
   "source": [
    "for this code. fix it . improve it. wrap into functions. and explain. remove unneeded code. tell me what you changed and did. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e69016e",
   "metadata": {},
   "source": [
    "# TODO Before you do anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "545e5fd9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T12:44:05.380093Z",
     "start_time": "2023-06-23T12:44:05.369565Z"
    }
   },
   "outputs": [],
   "source": [
    "#on windows: use the windows key. Then type anaconda . Open the anaconda power prompt as ADMINISTRATOR (right click). then conda activate base. #\n",
    "#Then conda create -n myenv python=3.8. then conda install -c conda-forge graph-tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6be0d53",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c91b5a26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-30T08:25:43.487273Z",
     "start_time": "2023-06-30T08:25:43.480547Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/yil1/opt/anaconda3/envs/P2P/bin/python'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check your python installation\n",
    "import sys\n",
    "sys.executable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73307d27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T18:26:41.549715Z",
     "start_time": "2023-06-29T18:26:41.545830Z"
    }
   },
   "outputs": [],
   "source": [
    "def install_missing_packages(package_names):\n",
    "    \"\"\"\n",
    "    Install Missing Packages\n",
    "\n",
    "    This function checks if a list of packages is already installed and installs any missing packages using pip.\n",
    "\n",
    "    Parameters:\n",
    "    - package_names (list): A list of package names to be installed.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "\n",
    "    Note: This function requires the `subprocess` and `importlib` modules to be imported.\n",
    "\n",
    "    Example Usage:\n",
    "    install_missing_packages(['h2o', 'numpy', 'pandas'])\n",
    "    \"\"\"\n",
    "    import importlib\n",
    "    import subprocess\n",
    "\n",
    "\n",
    "    for package_name in package_names:\n",
    "        try:\n",
    "            importlib.import_module(package_name)\n",
    "            print(f\"{package_name} package is already installed\")\n",
    "        except ImportError:\n",
    "            print(f\"{package_name} package not found, installing with pip...\")\n",
    "            subprocess.call(['pip', 'install', package_name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "818fc30d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T18:26:58.494145Z",
     "start_time": "2023-06-29T18:26:45.427345Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "networkx package not found, installing with pip...\n",
      "Collecting networkx\n",
      "  Using cached networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "Installing collected packages: networkx\n",
      "Successfully installed networkx-3.1\n",
      "gower package not found, installing with pip...\n",
      "Collecting gower\n",
      "  Using cached gower-0.1.2-py3-none-any.whl (5.2 kB)\n",
      "Requirement already satisfied: numpy in /Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages (from gower) (1.24.4)\n",
      "Requirement already satisfied: scipy in /Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages (from gower) (1.10.1)\n",
      "Installing collected packages: gower\n",
      "Successfully installed gower-0.1.2\n",
      "statsmodels package not found, installing with pip...\n",
      "Collecting statsmodels\n",
      "  Using cached statsmodels-0.14.0-cp38-cp38-macosx_11_0_arm64.whl (9.3 MB)\n",
      "Requirement already satisfied: numpy>=1.18 in /Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages (from statsmodels) (1.24.4)\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.4 in /Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages (from statsmodels) (1.10.1)\n",
      "Requirement already satisfied: pandas>=1.0 in /Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages (from statsmodels) (2.0.2)\n",
      "Collecting patsy>=0.5.2 (from statsmodels)\n",
      "  Using cached patsy-0.5.3-py2.py3-none-any.whl (233 kB)\n",
      "Requirement already satisfied: packaging>=21.3 in /Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages (from statsmodels) (23.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages (from pandas>=1.0->statsmodels) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages (from pandas>=1.0->statsmodels) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages (from pandas>=1.0->statsmodels) (2023.3)\n",
      "Requirement already satisfied: six in /Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages (from patsy>=0.5.2->statsmodels) (1.16.0)\n",
      "Installing collected packages: patsy, statsmodels\n",
      "Successfully installed patsy-0.5.3 statsmodels-0.14.0\n",
      "pyarrow package not found, installing with pip...\n",
      "Collecting pyarrow\n",
      "  Downloading pyarrow-12.0.1-cp38-cp38-macosx_11_0_arm64.whl (22.6 MB)\n",
      "\u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.6/22.6 MB 93.9 MB/s eta 0:00:00\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages (from pyarrow) (1.24.4)\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-12.0.1\n",
      "seaborn package not found, installing with pip...\n",
      "Collecting seaborn\n",
      "  Using cached seaborn-0.12.2-py3-none-any.whl (293 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.17 in /Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages (from seaborn) (1.24.4)\n",
      "Requirement already satisfied: pandas>=0.25 in /Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages (from seaborn) (2.0.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages (from seaborn) (3.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.40.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (23.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (5.2.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages (from pandas>=0.25->seaborn) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages (from pandas>=0.25->seaborn) (2023.3)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib!=3.6.1,>=3.1->seaborn) (3.11.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.12.2\n",
      "lime package not found, installing with pip...\n",
      "Collecting lime\n",
      "  Using cached lime-0.2.0.1-py3-none-any.whl\n",
      "Requirement already satisfied: matplotlib in /Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages (from lime) (3.7.1)\n",
      "Requirement already satisfied: numpy in /Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages (from lime) (1.24.4)\n",
      "Requirement already satisfied: scipy in /Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages (from lime) (1.10.1)\n",
      "Requirement already satisfied: tqdm in /Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages (from lime) (4.65.0)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in /Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages (from lime) (1.2.2)\n",
      "Collecting scikit-image>=0.12 (from lime)\n",
      "  Downloading scikit_image-0.21.0-cp38-cp38-macosx_12_0_arm64.whl (12.3 MB)\n",
      "\u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.3/12.3 MB 96.5 MB/s eta 0:00:00\n",
      "\u001b[?25hRequirement already satisfied: networkx>=2.8 in /Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages (from scikit-image>=0.12->lime) (3.1)\n",
      "Requirement already satisfied: pillow>=9.0.1 in /Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages (from scikit-image>=0.12->lime) (9.5.0)\n",
      "Collecting imageio>=2.27 (from scikit-image>=0.12->lime)\n",
      "  Downloading imageio-2.31.1-py3-none-any.whl (313 kB)\n",
      "\u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 313.2/313.2 kB 48.9 MB/s eta 0:00:00\n",
      "\u001b[?25hCollecting tifffile>=2022.8.12 (from scikit-image>=0.12->lime)\n",
      "  Using cached tifffile-2023.4.12-py3-none-any.whl (219 kB)\n",
      "Collecting PyWavelets>=1.1.1 (from scikit-image>=0.12->lime)\n",
      "  Using cached PyWavelets-1.4.1-cp38-cp38-macosx_11_0_arm64.whl (4.3 MB)\n",
      "Requirement already satisfied: packaging>=21 in /Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages (from scikit-image>=0.12->lime) (23.0)\n",
      "Collecting lazy_loader>=0.2 (from scikit-image>=0.12->lime)\n",
      "  Using cached lazy_loader-0.2-py3-none-any.whl (8.6 kB)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages (from scikit-learn>=0.18->lime) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages (from scikit-learn>=0.18->lime) (3.1.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages (from matplotlib->lime) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages (from matplotlib->lime) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages (from matplotlib->lime) (4.40.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages (from matplotlib->lime) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages (from matplotlib->lime) (3.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages (from matplotlib->lime) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages (from matplotlib->lime) (5.2.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib->lime) (3.11.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: six>=1.5 in /Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib->lime) (1.16.0)\n",
      "Installing collected packages: tifffile, PyWavelets, lazy_loader, imageio, scikit-image, lime\n",
      "Successfully installed PyWavelets-1.4.1 imageio-2.31.1 lazy_loader-0.2 lime-0.2.0.1 scikit-image-0.21.0 tifffile-2023.4.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages/shap/utils/_clustering.py:35: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _pt_shuffle_rec(i, indexes, index_mask, partition_tree, M, pos):\n",
      "/Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages/shap/utils/_clustering.py:54: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def delta_minimization_order(all_masks, max_swap_size=100, num_passes=2):\n",
      "/Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages/shap/utils/_clustering.py:63: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _reverse_window(order, start, length):\n",
      "/Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages/shap/utils/_clustering.py:69: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _reverse_window_score_gain(masks, order, start, length):\n",
      "/Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages/shap/utils/_clustering.py:77: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _mask_delta_score(m1, m2):\n",
      "/Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages/shap/links.py:5: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def identity(x):\n",
      "/Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages/shap/links.py:10: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _identity_inverse(x):\n",
      "/Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages/shap/links.py:15: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def logit(x):\n",
      "/Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages/shap/links.py:20: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _logit_inverse(x):\n",
      "/Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages/shap/utils/_masked_model.py:363: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _build_fixed_single_output(averaged_outs, last_outs, outputs, batch_positions, varying_rows, num_varying_rows, link, linearizing_weights):\n",
      "/Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages/shap/utils/_masked_model.py:385: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _build_fixed_multi_output(averaged_outs, last_outs, outputs, batch_positions, varying_rows, num_varying_rows, link, linearizing_weights):\n",
      "/Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages/shap/utils/_masked_model.py:428: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _init_masks(cluster_matrix, M, indices_row_pos, indptr):\n",
      "/Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages/shap/utils/_masked_model.py:439: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _rec_fill_masks(cluster_matrix, indices_row_pos, indptr, indices, M, ind):\n",
      "/Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages/shap/maskers/_tabular.py:186: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _single_delta_mask(dind, masked_inputs, last_mask, data, x, noop_code):\n",
      "/Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages/shap/maskers/_tabular.py:197: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _delta_masking(masks, x, curr_delta_inds, varying_rows_out,\n",
      "/Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages/shap/maskers/_image.py:175: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _jit_build_partition_tree(xmin, xmax, ymin, ymax, zmin, zmax, total_ywidth, total_zwidth, M, clustering, q):\n",
      "/Users/yil1/opt/anaconda3/envs/P2P/lib/python3.8/site-packages/shap/explainers/_partition.py:676: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def lower_credit(i, value, M, values, clustering):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shap package is already installed\n"
     ]
    }
   ],
   "source": [
    "package_list = [\"networkx\", \"gower\",\"statsmodels\",\"pyarrow\",\"seaborn\", \"lime\", \"shap\"]\n",
    "install_missing_packages(package_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6298fd6e",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3522e83",
   "metadata": {},
   "source": [
    "Read the raw data from bondora.\n",
    "Do some preprocessing\n",
    "Save it as cleaned data in feather format.\n",
    "Then sample a subset and save as cvs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f6c989c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T18:27:00.594242Z",
     "start_time": "2023-06-29T18:27:00.566343Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "clean_data(df):\n",
    "This code defines a function called `clean_data`, which takes a DataFrame (usually a DataFrame object in the pandas library) as input and performs a series of data cleaning operations on it. The specific cleaning steps are as follows:\n",
    "\n",
    "1. **Delete missing values**: Use the `dropna()` method to remove rows containing missing values.\n",
    "\n",
    "2. **Filter specific rows**: keep the rows whose `lang.1` column is equal to 1, and reset the index.\n",
    "\n",
    "3. **Delete 'lang' column**: Delete all columns containing \"lang\".\n",
    "\n",
    "4. **Delete Date Columns**: Delete `date.start` and `date.end` columns as they are considered irrelevant.\n",
    "\n",
    "5. **Remove Look-Bias Variables**: Define a list containing look-ahead-bias variables and use the `drop()` method to drop those columns from the DataFrame.\n",
    "\n",
    "6. **Delete Duplicate Income Variables**: Delete all columns that contain \"inc.\" and do not contain \".no\".\n",
    "\n",
    "7. **Delete Dummy Variables**: Defines a list of dummy variables to delete and removes these columns from the DataFrame.\n",
    "\n",
    "8. **Generate Correlation Matrix**: Use the `corr()` method to generate the correlation matrix of DataFrame.\n",
    "\n",
    "9. **Choose the upper triangle of the correlation matrix**: To avoid multicollinearity, select the upper triangle of the correlation matrix.\n",
    "\n",
    "10. **Find High Correlation Columns**: Find columns that have a correlation higher than 0.95 with other columns.\n",
    "\n",
    "11. **Delete High Correlation Columns**: Delete the high correlation columns found in step 10.\n",
    "\n",
    "Finally, the function returns the cleaned DataFrame.\n",
    "\n",
    "The purpose of this code is to prepare the data for subsequent analysis or modeling. It cleans data by removing missing values, filtering specific rows, removing irrelevant or biased columns, and handling multicollinearity.\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import random\n",
    "\n",
    "# Define a function to calculate variance inflation factor (VIF) for all variables in a given DataFrame.\n",
    "def calculate_vif(df):\n",
    "    \"\"\"Calculates variance inflation factor for all columns in df. It should contain \n",
    "    only exogeneous variables.\"\"\"\n",
    "    \n",
    "    # Create an empty DataFrame\n",
    "    vif = pd.DataFrame()\n",
    "    \n",
    "    # Calculate VIF for every column (variable) in the DataFrame\n",
    "    vif[\"VIF Factor\"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n",
    "    \n",
    "    # Store column names\n",
    "    vif[\"features\"] = df.columns\n",
    "    \n",
    "    # Sort by VIF Factor in descending order\n",
    "    vif = vif.sort_values(\"VIF Factor\", ascending=False)\n",
    "    return vif\n",
    "\n",
    "# Define a function to clean the DataFrame, removing irrelevant and problematic features.\n",
    "def clean_data(df):\n",
    "    \"\"\"\n",
    "    Clean the given DataFrame, drop unnecessary columns, handle missing data, remove biased variables, \n",
    "    avoid multicollinearity by checking correlation, and keep only relevant columns.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Drop rows with missing values\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "    # Filter for only rows where 'lang.1' equals 1, and drop 'lang' columns\n",
    "    df = df[df[\"lang.1\"] == 1].reset_index(drop=True)\n",
    "    df = df.drop([x for x in df.columns if \"lang\" in x], axis=1)\n",
    "\n",
    "    # Drop date columns as they are not considered relevant\n",
    "    df = df.drop([\"date.start\", \"date.end\"], axis=1)\n",
    "\n",
    "    # List of forward-looking biased variables\n",
    "    fwl_bias = [\n",
    "        \"return\",\n",
    "        \"RR1\",\n",
    "        \"RR2.Mean\",\n",
    "        \"RR2.Median\",\n",
    "        \"RR2.WMean\",\n",
    "        \"NPRP\",\n",
    "        \"NPRA\",\n",
    "        \"FVCI\",\n",
    "        \"FVCI.Mean\",\n",
    "        \"FVCI.Median\",\n",
    "        \"FVCI.WMean\",\n",
    "    ]\n",
    "    # Drop these forward-looking biased variables\n",
    "    df = df.drop(fwl_bias, axis=1)\n",
    "\n",
    "    # Drop duplicate income variables, keep only those with '.no' in name\n",
    "    df = df.drop([x for x in df.columns if \"inc.\" in x and \".no\" in x], axis=1)\n",
    "\n",
    "    # List of dummy variables to drop\n",
    "    dummies_to_drop = [\"AA\", \"educ.6\", \"em.dur.5p\", \"use.m\", \"ver.2\", \"Mining\", \"Utilities\"]\n",
    "    df = df.drop(dummies_to_drop, axis=1)\n",
    "\n",
    "    # Generate a correlation matrix of the DataFrame\n",
    "    corr_matrix = df.corr().abs()\n",
    "    \n",
    "    # Select upper triangle of correlation matrix\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    \n",
    "    # Find columns with correlation greater than 0.95\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "    \n",
    "    # Drop these columns from the DataFrame\n",
    "    df = df.drop(df[to_drop], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_balanced_sample(df, n, replace = False):\n",
    "    \"\"\"Create a balanced sample of size 2n from df. Ensures that the sample contains an equal number of instances for each class.\"\"\"\n",
    "    \n",
    "    # Draw a random sample of size 'n' from the non-default class (default = 0) and from the default class (default = 1)\n",
    "    # replace=True allows for resampling\n",
    "    # Random state ensures reproducibility\n",
    "    df_sample = pd.concat(\n",
    "        [\n",
    "            df[df[\"default\"] == 0].sample(n=n, random_state=1, replace=replace),\n",
    "            df[df[\"default\"] == 1].sample(n=n, random_state=1, replace=replace),\n",
    "        ]\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    return df_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "010f5859",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T18:27:01.370549Z",
     "start_time": "2023-06-29T18:27:01.366658Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def generate_file_path(file_name):\n",
    "    \"\"\"\n",
    "    Generate a file path for a file located one directory level up.\n",
    "\n",
    "    Parameters:\n",
    "    file_name (str): The name of the file including any subdirectories from the parent directory.\n",
    "\n",
    "    Returns:\n",
    "    file_path (str): The full path to the file.\n",
    "    \"\"\"\n",
    "    # Get the current working directory\n",
    "    cwd = os.getcwd()\n",
    "\n",
    "    # Get the parent directory\n",
    "    parent_dir = os.path.dirname(cwd)\n",
    "\n",
    "    # Define the file path by joining the parent directory path with the file name\n",
    "    file_path = os.path.join(parent_dir, file_name)\n",
    "\n",
    "    return file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64968278",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T18:27:03.960622Z",
     "start_time": "2023-06-29T18:27:02.268554Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yil1/GitHub/P2P-Model-Bondora/data//Bondora_feather\n"
     ]
    }
   ],
   "source": [
    "# Load original dataset\n",
    "file_path = generate_file_path(\"data//Bondora_feather\")\n",
    "print(file_path)\n",
    "# Load the dataset\n",
    "df = pd.read_feather(file_path)\n",
    "\n",
    "# Clean data\n",
    "df_clean = clean_data(df)\n",
    "\n",
    "# Save cleaned data\n",
    "file_path_cleaned = generate_file_path(\"data//Bondora_clean.feather\")\n",
    "df_clean.to_feather(file_path_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bcae6da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T12:46:05.150017Z",
     "start_time": "2023-06-23T12:46:05.138447Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32469, 155)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecfab23a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T12:46:06.111349Z",
     "start_time": "2023-06-23T12:46:06.094508Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "default\n",
       "0.0    20241\n",
       "1.0    12228\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean['default'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a162500b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T12:46:08.330756Z",
     "start_time": "2023-06-23T12:46:08.327286Z"
    }
   },
   "outputs": [],
   "source": [
    "#Here, we create a varible to control sample size\n",
    "num_default_samples=12000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ec3b1e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T12:46:09.818388Z",
     "start_time": "2023-06-23T12:46:09.760460Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create balanced sample and save\n",
    "df_sample = create_balanced_sample(df_clean, num_default_samples, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91702a9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T12:46:10.606341Z",
     "start_time": "2023-06-23T12:46:10.568698Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yil1/GitHub/P2P-Model-Bondora/data//Bondora_sample(24000).feather\n"
     ]
    }
   ],
   "source": [
    "#save the file\n",
    "file_path_cleaned_sampled = generate_file_path(\n",
    "    \"data//Bondora_sample(\"+str(num_default_samples*2)+\").feather\")\n",
    "df_sample.to_feather(file_path_cleaned_sampled)\n",
    "print(file_path_cleaned_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32f2cb13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T12:46:12.670416Z",
     "start_time": "2023-06-23T12:46:12.625160Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>default</th>\n",
       "      <th>new</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Interest</th>\n",
       "      <th>MonthlyPayment</th>\n",
       "      <th>DebtToIncome</th>\n",
       "      <th>NoOfPreviousLoansBeforeLoan</th>\n",
       "      <th>AmountOfPreviousLoansBeforeLoan</th>\n",
       "      <th>time</th>\n",
       "      <th>...</th>\n",
       "      <th>no.previous.loan.04</th>\n",
       "      <th>no.previous.loan.05</th>\n",
       "      <th>no.previous.loan.06</th>\n",
       "      <th>no.previous.loan.07</th>\n",
       "      <th>no.previous.repay.00</th>\n",
       "      <th>no.previous.repay.01</th>\n",
       "      <th>previous.repay.l</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.11</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.846</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.22</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.845</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.86</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.846</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.846</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.85</td>\n",
       "      <td>1</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>1.840</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32464</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.41</td>\n",
       "      <td>3.433342</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5</td>\n",
       "      <td>9886.0</td>\n",
       "      <td>4.794</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32465</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.36</td>\n",
       "      <td>3.794815</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1275.0</td>\n",
       "      <td>4.797</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32466</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.12</td>\n",
       "      <td>3.684118</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>12120.0</td>\n",
       "      <td>4.798</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32467</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.63</td>\n",
       "      <td>5.477802</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>5021.0</td>\n",
       "      <td>4.798</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32468</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.49</td>\n",
       "      <td>3.004692</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7</td>\n",
       "      <td>9221.0</td>\n",
       "      <td>4.802</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.969791</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32469 rows × 155 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       default  new  Age  Gender  Interest  MonthlyPayment  DebtToIncome   \n",
       "0          1.0    1   30     1.0     28.00        0.000000         19.11  \\\n",
       "1          1.0    1   39     0.0     28.00        0.000000         45.22   \n",
       "2          1.0    1   32     0.0     28.00        0.000000         44.86   \n",
       "3          0.0    1   27     0.0     19.00        0.000000         10.70   \n",
       "4          0.0    0   31     0.0     16.00        0.000000         15.85   \n",
       "...        ...  ...  ...     ...       ...             ...           ...   \n",
       "32464      0.0    0   22     0.0     18.41        3.433342          0.00   \n",
       "32465      0.0    1   29     0.0     16.36        3.794815          0.00   \n",
       "32466      0.0    0   32     0.0     17.12        3.684118          0.00   \n",
       "32467      0.0    0   46     1.0     25.63        5.477802          0.00   \n",
       "32468      0.0    0   45     0.0     22.49        3.004692          0.00   \n",
       "\n",
       "       NoOfPreviousLoansBeforeLoan  AmountOfPreviousLoansBeforeLoan   time   \n",
       "0                                0                              0.0  1.846  \\\n",
       "1                                0                              0.0  1.845   \n",
       "2                                0                              0.0  1.846   \n",
       "3                                0                              0.0  1.846   \n",
       "4                                1                           3000.0  1.840   \n",
       "...                            ...                              ...    ...   \n",
       "32464                            5                           9886.0  4.794   \n",
       "32465                            1                           1275.0  4.797   \n",
       "32466                            3                          12120.0  4.798   \n",
       "32467                            3                           5021.0  4.798   \n",
       "32468                            7                           9221.0  4.802   \n",
       "\n",
       "       ...  no.previous.loan.04  no.previous.loan.05  no.previous.loan.06   \n",
       "0      ...                  0.0                  0.0                  0.0  \\\n",
       "1      ...                  0.0                  0.0                  0.0   \n",
       "2      ...                  0.0                  0.0                  0.0   \n",
       "3      ...                  0.0                  0.0                  0.0   \n",
       "4      ...                  0.0                  0.0                  0.0   \n",
       "...    ...                  ...                  ...                  ...   \n",
       "32464  ...                  0.0                  1.0                  0.0   \n",
       "32465  ...                  0.0                  0.0                  0.0   \n",
       "32466  ...                  0.0                  0.0                  0.0   \n",
       "32467  ...                  0.0                  0.0                  0.0   \n",
       "32468  ...                  0.0                  0.0                  0.0   \n",
       "\n",
       "       no.previous.loan.07  no.previous.repay.00  no.previous.repay.01   \n",
       "0                      0.0                   1.0                   0.0  \\\n",
       "1                      0.0                   1.0                   0.0   \n",
       "2                      0.0                   1.0                   0.0   \n",
       "3                      0.0                   1.0                   0.0   \n",
       "4                      0.0                   1.0                   0.0   \n",
       "...                    ...                   ...                   ...   \n",
       "32464                  0.0                   1.0                   0.0   \n",
       "32465                  0.0                   1.0                   0.0   \n",
       "32466                  0.0                   1.0                   0.0   \n",
       "32467                  0.0                   1.0                   0.0   \n",
       "32468                  1.0                   0.0                   1.0   \n",
       "\n",
       "       previous.repay.l  A  B  C  \n",
       "0              0.000000  0  1  0  \n",
       "1              0.000000  0  1  0  \n",
       "2              0.000000  0  0  1  \n",
       "3              0.000000  0  0  1  \n",
       "4              0.000000  1  0  0  \n",
       "...                 ... .. .. ..  \n",
       "32464          0.000000  0  1  0  \n",
       "32465          0.000000  0  1  0  \n",
       "32466          0.000000  0  1  0  \n",
       "32467          0.000000  0  0  1  \n",
       "32468          6.969791  0  0  1  \n",
       "\n",
       "[32469 rows x 155 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generating data frame for the entire datasample\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3eb1c610",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T12:46:19.730464Z",
     "start_time": "2023-06-23T12:46:19.722105Z"
    }
   },
   "outputs": [],
   "source": [
    "# function to save a pandas dataframe as csv to disc.\n",
    "def save_df_to_csv(df, file_path):\n",
    "    \"\"\"\n",
    "    This function saves a pandas DataFrame to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The DataFrame to save.\n",
    "    file_path (str): The file path where to save the DataFrame, including the filename.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    df.to_csv(file_path, index=False)  # Set index=False to not save row indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7d8adcd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T12:46:21.903640Z",
     "start_time": "2023-06-23T12:46:20.998251Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yil1/GitHub/P2P-Model-Bondora/data//Bondora_clean.csv\n"
     ]
    }
   ],
   "source": [
    "# save the pandas.dataframe \"df_clean\" as csv.\n",
    "file_path_full_data = generate_file_path(\"data//Bondora_clean.csv\")\n",
    "print(file_path_full_data)\n",
    "# Usage of the function\n",
    "save_df_to_csv(df_clean, file_path_full_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c218bb96",
   "metadata": {},
   "source": [
    "## Computing the summary stats for the raw bondora feather data set (most informative features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8116cf8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T12:46:25.388819Z",
     "start_time": "2023-06-23T12:46:25.379761Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def compute_summary_stats(df, cols):\n",
    "    \"\"\"\n",
    "    This function computes summary statistics for specified columns in a pandas dataframe.\n",
    "\n",
    "    Args:\n",
    "    df (pandas.DataFrame): The dataframe on which to compute summary statistics.\n",
    "    cols (list): A list of column names for which to compute summary statistics.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A dataframe with summary statistics for the specified columns.\n",
    "\n",
    "    Example:\n",
    "    summary_stats = compute_summary_stats(df, ['liab.I', 'inc.total', 'MonthlyPayment', 'log.amount', 'time', 'Interest', 'AmountOfPreviousLoansBeforeLoan', 'NoOfPreviousLoansBeforeLoan', 'Age'])\n",
    "    print(summary_stats)\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if all columns exist in dataframe\n",
    "    for col in cols:\n",
    "        if col not in df.columns:\n",
    "            print(f'Column {col} does not exist in the dataframe.')\n",
    "            return None\n",
    "\n",
    "    # Compute summary statistics\n",
    "    summary_stats = df[cols].describe()\n",
    "\n",
    "    return summary_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e60de669",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T12:46:27.476589Z",
     "start_time": "2023-06-23T12:46:27.440277Z"
    }
   },
   "outputs": [],
   "source": [
    "# create summary_stats_IF data frame\n",
    "summary_stats_IF_raw = compute_summary_stats(df_clean, ['liab.l', 'inc.total', 'MonthlyPayment', 'log.amount', 'time', 'Interest', 'AmountOfPreviousLoansBeforeLoan', 'NoOfPreviousLoansBeforeLoan', 'Age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b26031",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.395Z"
    }
   },
   "outputs": [],
   "source": [
    "def df_to_latex(df):\n",
    "    \"\"\"\n",
    "    This function converts a pandas DataFrame into a LaTeX table.\n",
    "\n",
    "    Args:\n",
    "    df (pandas.DataFrame): The dataframe to convert to LaTeX.\n",
    "\n",
    "    Returns:\n",
    "    str: A string of LaTeX code for a table with the data from the DataFrame.\n",
    "\n",
    "    Example:\n",
    "    latex_code = df_to_latex(summary_stats)\n",
    "    print(latex_code)\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert DataFrame to LaTeX\n",
    "    latex_code = df.to_latex()\n",
    "\n",
    "    return latex_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c353c217",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.415Z"
    }
   },
   "outputs": [],
   "source": [
    "# create tabular summary stats for IF on raw data set\n",
    "df_to_latex(summary_stats_IF_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82950439",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.416Z"
    }
   },
   "outputs": [],
   "source": [
    "# retrieving the number of non-defaulted vs. defaulted loans\n",
    "def count_defaults(df):\n",
    "    \"\"\"\n",
    "    This function counts and prints the number of defaulted loans in a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame containing the loan data. It must contain a 'default' column \n",
    "                    with binary values: 1 for default and 0 otherwise.\n",
    "\n",
    "    Returns:\n",
    "    None. The function directly prints the number of defaulted loans.\n",
    "    \"\"\"\n",
    "    \n",
    "    num_defaults = df['default'].sum()\n",
    "    print(f\"The number of loans that have defaulted is: {num_defaults}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e8fd1a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.418Z"
    }
   },
   "outputs": [],
   "source": [
    "# Call function counts_default with your df_clean\n",
    "count_defaults(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ad2266",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.419Z"
    }
   },
   "outputs": [],
   "source": [
    "df_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fa0a0b",
   "metadata": {},
   "source": [
    "# Compute Gowers distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c263a3",
   "metadata": {},
   "source": [
    "Gower's distance is a measure of similarity between two data objects in a dataset. It was proposed by J.C. Gower in 1971 and is particularly useful when dealing with mixed data types, such as datasets that have a combination of numerical and categorical data.\n",
    "\n",
    "Gower's distance calculates the similarity between objects based on a radial basis function between each variable of the objects. For numerical variables, the individual variable difference between two objects is divided by the range of that variable. For categorical variables, the difference is 0 if the category is the same and 1 otherwise. The individual similarities are then combined into an overall similarity.\n",
    "\n",
    "\n",
    "If you're dealing with datasets that are purely numerical or categorical, there are other distance measures that might be more appropriate:\n",
    "\n",
    "For numerical data, you can use Euclidean distance, Manhattan distance, or Minkowski distance. These are available in the scipy.spatial.distance module.\n",
    "\n",
    "For categorical data, you can use Hamming distance or Jaccard similarity. Hamming distance is available in the scipy.spatial.distance module, and Jaccard similarity can be calculated using the sklearn.metrics.jaccard_score function.\n",
    "\n",
    "Remember, the choice of distance measure can significantly impact the results of your analysis, so it's important to understand the properties of each measure and choose the one that's most appropriate for your data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f23c4b",
   "metadata": {},
   "source": [
    "## Another Gower implementation - Not used at the moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d51f6bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T12:46:38.298264Z",
     "start_time": "2023-06-23T12:46:38.290254Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import DistanceMetric\n",
    "\n",
    "\n",
    "def gower_distance(X):\n",
    "    \"\"\"\n",
    "    This function expects a pandas dataframe as input\n",
    "    The data frame is to contain the features along the columns. Based on these features a\n",
    "    distance matrix will be returned which will contain the pairwise gower distance between the rows\n",
    "    All variables of object type will be treated as nominal variables and the others will be treated as \n",
    "    numeric variables.\n",
    "\n",
    "    Distance metrics used for:\n",
    "\n",
    "    Nominal variables: Dice distance (https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient)\n",
    "    Numeric variables: Manhattan distance normalized by the range of the variable (https://en.wikipedia.org/wiki/Taxicab_geometry)\n",
    "    \"\"\"\n",
    "    individual_variable_distances = []\n",
    "\n",
    "    for i in range(X.shape[1]):\n",
    "        feature = X.iloc[:,[i]]\n",
    "        if feature.dtypes[0] == object:\n",
    "            feature_dist = DistanceMetric.get_metric('dice').pairwise(pd.get_dummies(feature))\n",
    "        else:\n",
    "            feature_dist = DistanceMetric.get_metric('manhattan').pairwise(feature) / np.ptp(feature.values)\n",
    "            \n",
    "        individual_variable_distances.append(feature_dist)\n",
    "\n",
    "    return np.array(individual_variable_distances).mean(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2936d98",
   "metadata": {},
   "source": [
    "## Calculate and save gowers distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19655a19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T12:46:41.471327Z",
     "start_time": "2023-06-23T12:46:41.458008Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gower\n",
    "\n",
    "\n",
    "def calculate_and_save_gowers_distance(dataframe, output_file_path):\n",
    "    \"\"\"\n",
    "    Calculates Gower's distance matrix for a given DataFrame and saves the result as a numpy array.\n",
    "\n",
    "    Parameters:\n",
    "    dataframe (pandas.DataFrame): The input DataFrame.\n",
    "    output_file_path (str): The path where the resulting numpy array should be saved.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Drop the 'default' column to keep only features\n",
    "        dataframe = dataframe.drop([\"default\"], axis=1)\n",
    "\n",
    "        # Identify dummy columns (faster for gower calculations)\n",
    "        dummy_columns = [\n",
    "            column for column in dataframe.columns\n",
    "            if ((dataframe[column] == 0) | (dataframe[column] == 1)).all()\n",
    "        ]\n",
    "        categorical_variables = [\n",
    "            column in dummy_columns for column in dataframe.columns\n",
    "        ]\n",
    "\n",
    "        # Calculate Gower's distance\n",
    "        distance_matrix = gower.gower_matrix(\n",
    "            dataframe, cat_features=categorical_variables)\n",
    "\n",
    "        # Save the distance matrix as a numpy array\n",
    "        np.save(output_file_path, distance_matrix)\n",
    "\n",
    "        print(f\"Distance matrix saved successfully at {output_file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    return distance_matrix\n",
    "\n",
    "\n",
    "# Changes made:\n",
    "\n",
    "# 1. Renamed the function to `calculate_and_save_gowers_distance` for clarity.\n",
    "# 2. Added a try-except block to handle potential errors during execution.\n",
    "# 3. Added comments to explain each step of the process.\n",
    "# 4. Improved the print statement to include the output file path for better tracking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebf1417",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T19:20:35.493840Z",
     "start_time": "2023-05-19T19:20:35.433190Z"
    }
   },
   "source": [
    "### Gower example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3107f27b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T12:46:43.252692Z",
     "start_time": "2023-06-23T12:46:43.234463Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gower\n",
    "\n",
    "# Create a simple DataFrame\n",
    "data = {'feature1': ['A', 'B', 'B', 'A', 'B'],\n",
    "        'feature2': [1, 2, 3, 2, 1],\n",
    "        'feature3': [5.0, 4.1, 3.2, 2.3, 1.4]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define which columns are categorical\n",
    "cat_features = [True, False, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "356ab6f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T12:46:44.979760Z",
     "start_time": "2023-06-23T12:46:44.971239Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.5833333  0.8333333  0.41666666 0.6666667 ]\n",
      " [0.5833333  0.         0.25       0.5        0.41666666]\n",
      " [0.8333333  0.25       0.         0.5833333  0.5       ]\n",
      " [0.41666666 0.5        0.5833333  0.         0.5833333 ]\n",
      " [0.6666667  0.41666666 0.5        0.5833333  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate Gower's distance\n",
    "gower_distance_example = gower.gower_matrix(df, cat_features=cat_features)\n",
    "\n",
    "print(gower_distance_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333e61ca",
   "metadata": {},
   "source": [
    "#### Compare two functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f723bf36",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.426Z"
    }
   },
   "outputs": [],
   "source": [
    "gower_distance(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0145d6b6",
   "metadata": {},
   "source": [
    "### Compute adjacency, distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0546341d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.427Z"
    }
   },
   "outputs": [],
   "source": [
    "# Usage\n",
    "# Define the file path for the distance matrix\n",
    "sample_data_path = generate_file_path(\"data//Bondora_sample(\"+str(num_default_samples*2)+\").feather\")\n",
    "df_sample = pd.read_feather(sample_data_path)\n",
    "print(sample_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020113a5",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.428Z"
    }
   },
   "outputs": [],
   "source": [
    "dist_matrix_path = generate_file_path(\"data//Dist_matrix(\"+str(num_default_samples*2)+\").npy\")\n",
    "print(dist_matrix_path)\n",
    "distance_matrix = calculate_and_save_gowers_distance(df_sample, dist_matrix_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d058cab4",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.429Z"
    }
   },
   "outputs": [],
   "source": [
    "distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4facaa56",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.430Z"
    }
   },
   "outputs": [],
   "source": [
    "distance_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c47bbb1",
   "metadata": {},
   "source": [
    "## Visualize distance matrix\n",
    "Do not run this part when unnecessary. It is time consuming and results in errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47817a4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T12:46:49.238952Z",
     "start_time": "2023-06-23T12:46:49.206353Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'distance_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n\u001b[0;32m----> 5\u001b[0m sns\u001b[38;5;241m.\u001b[39mheatmap(\u001b[43mdistance_matrix\u001b[49m, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mviridis\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'distance_matrix' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(distance_matrix, cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e63f7e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.433Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "linked = linkage(distance_matrix, 'single')\n",
    "dendrogram(linked)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f84339",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.434Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "\n",
    "model = MDS(n_components=2, dissimilarity='precomputed')\n",
    "out = model.fit_transform(distance_matrix)\n",
    "plt.scatter(out[:, 0], out[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3ff96a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.435Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(distance_matrix.flatten(), bins=200)\n",
    "plt.xlabel('Distance')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0864c001",
   "metadata": {},
   "source": [
    "# Add centrality measures\n",
    "more on graph-tools.\n",
    "https://robert-haas.github.io/gravis-docs/code/examples/external_tools/graph-tool.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa81ea2b",
   "metadata": {},
   "source": [
    "## old version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2f5914",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T10:09:10.407564Z",
     "start_time": "2023-05-19T10:09:10.350399Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from graph_tool.all import *\n",
    "import time\n",
    "\n",
    "#======================================================================================\n",
    "#GRAPH TOOL - MST\n",
    "#======================================================================================\n",
    "\n",
    "\n",
    "def matrix_to_graph_tool(adj):\n",
    "    # Extract index and weights\n",
    "    idx = np.nonzero(np.triu(adj, 1))\n",
    "    weights = adj[idx]\n",
    "    # Create graph\n",
    "    g = Graph()\n",
    "    g.add_edge_list(np.transpose(idx))\n",
    "    # Add weights as an edge propetyMap\n",
    "    edge_weight = g.new_edge_property(\"double\")\n",
    "    edge_weight.a = weights\n",
    "    g.edge_properties[\"edge_weight\"] = edge_weight\n",
    "    return g\n",
    "\n",
    "\n",
    "g = matrix_to_graph_tool(np.load(\"data/dist_matrix_10k.npy\"))\n",
    "tree = min_spanning_tree(g, weights=g.ep.edge_weight)\n",
    "g.set_edge_filter(tree)\n",
    "\n",
    "#======================================================================================\n",
    "#CENTRALITY MEASURES\n",
    "#======================================================================================\n",
    "\n",
    "pr_w = pagerank(g, weight=g.ep.edge_weight).a\n",
    "bw_w = betweenness(g, weight=g.ep.edge_weight)[0].a\n",
    "cl_w = closeness(g, weight=g.ep.edge_weight).a\n",
    "ev_w = eigenvector(g, weight=g.ep.edge_weight)[1].a\n",
    "kt_w = katz(g, weight=g.ep.edge_weight).a\n",
    "hits_aut_w = hits(g, weight=g.ep.edge_weight)[1].a\n",
    "hits_hub_w = hits(g, weight=g.ep.edge_weight)[2].a\n",
    "\n",
    "ls = [\n",
    "    pr_w,\n",
    "    bw_w,\n",
    "    cl_w,\n",
    "    ev_w,\n",
    "    kt_w,\n",
    "    hits_aut_w,\n",
    "    hits_hub_w,\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(np.row_stack(ls).T)\n",
    "\n",
    "df.columns = [\n",
    "    \"pagerank\",\n",
    "    \"betweenness\",\n",
    "    \"closeness\",\n",
    "    \"eigenvector\",\n",
    "    \"katz\",\n",
    "    \"authority\",\n",
    "    \"hub\",\n",
    "]\n",
    "\n",
    "#Merge with original dataset\n",
    "orig_df = pd.read_feather(\"data/bondora_clean_10k.feather\")\n",
    "final_df = pd.concat([orig_df, df], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e516943",
   "metadata": {},
   "source": [
    "## Add centrality measures\n",
    "Wrapped the graph creation and centrality measure calculation into separate functions for better readability and reusability.\n",
    "Removed unnecessary list ls. Instead, created the DataFrame directly using a dictionary.\n",
    "Added docstrings to the functions to explain what they do.\n",
    "Removed the unused time import.\n",
    "Added comments to explain each step of the process.\n",
    "Used pd.concat() to merge the original DataFrame with the centrality measures DataFrame. This is more efficient and readable than creating a new DataFrame and then adding columns to it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09154bc",
   "metadata": {},
   "source": [
    "In a weighted graph, the weight of an edge can represent various things depending on the context of the problem. It could represent the strength of the connection between the nodes, the distance between the nodes, the cost of moving from one node to another, etc.\n",
    "\n",
    "The way the weight of an edge affects the centrality measures depends on the specific measure:\n",
    "\n",
    "1. **PageRank**: In the context of PageRank, the weight of an edge can be interpreted as the probability of a random walk following the edge. A higher weight means that it's more likely for the random walk to follow the edge. Therefore, nodes connected by high-weight edges will have a higher PageRank centrality.\n",
    "\n",
    "2. **Betweenness**: In the context of betweenness centrality, the weight of an edge can be interpreted as the cost of traversing the edge. A lower weight means that it's less costly to traverse the edge. Therefore, nodes connected by low-weight edges will have a higher betweenness centrality.\n",
    "\n",
    "3. **Closeness**: In the context of closeness centrality, the weight of an edge can be interpreted as the distance between the nodes. A lower weight means that the nodes are closer to each other. Therefore, nodes connected by low-weight edges will have a higher closeness centrality.\n",
    "\n",
    "4. **Eigenvector**: In the context of eigenvector centrality, the weight of an edge contributes to the centrality of the nodes it connects. A higher weight means that the nodes will have a higher eigenvector centrality.\n",
    "\n",
    "5. **Katz**: In the context of Katz centrality, the weight of an edge contributes to the centrality of the nodes it connects. A higher weight means that the nodes will have a higher Katz centrality.\n",
    "\n",
    "6. **HITS**: In the context of HITS, the weight of an edge contributes to the authority and hub scores of the nodes it connects. A higher weight means that the nodes will have a higher authority and hub scores.\n",
    "\n",
    "In all these measures, the weight of an edge is used to quantify the importance of the edge in the graph. The exact interpretation of the weight depends on the context of the problem and the specific centrality measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52260458",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.437Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from graph_tool.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9e7761",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.438Z"
    }
   },
   "outputs": [],
   "source": [
    "def matrix_to_graph_tool(adj):\n",
    "    \"\"\"\n",
    "    Convert adjacency matrix to graph using graph-tool.\n",
    "\n",
    "    Parameters:\n",
    "    adj (numpy.ndarray): The adjacency matrix.\n",
    "\n",
    "    Returns:\n",
    "    g (graph_tool.Graph): The graph.\n",
    "    \"\"\"\n",
    "    # Extract index and weights from the adjacency matrix.\n",
    "    # np.nonzero(np.triu(adj, 1)) returns the indices of the upper triangle of the matrix,\n",
    "    # excluding the diagonal. This is because for an undirected graph, the adjacency matrix\n",
    "    # is symmetric, and we only need to consider half of the matrix to get all the edges.\n",
    "    idx = np.nonzero(np.triu(adj, 1))\n",
    "\n",
    "    # Get the weights of the edges from the adjacency matrix.\n",
    "    weights = adj[idx]\n",
    "\n",
    "    # Create an empty graph.\n",
    "    g = Graph()\n",
    "\n",
    "    # Add edges to the graph. np.transpose(idx) gives a 2D array where each row is the indices\n",
    "    # of the two vertices of an edge.\n",
    "    g.add_edge_list(np.transpose(idx))\n",
    "\n",
    "    # Create an edge property map for the weights of the edges.\n",
    "    edge_weight = g.new_edge_property(\"double\")\n",
    "\n",
    "    # Assign the weights to the edge property map.\n",
    "    edge_weight.a = weights\n",
    "\n",
    "    # Add the edge property map to the graph.\n",
    "    g.edge_properties[\"edge_weight\"] = edge_weight\n",
    "\n",
    "    return g\n",
    "\n",
    "def calculate_centrality_measures(g):\n",
    "    \"\"\"\n",
    "    Calculate various centrality measures for the graph.\n",
    "\n",
    "    Parameters:\n",
    "    g (graph_tool.Graph): The graph.\n",
    "\n",
    "    Returns:\n",
    "    df (pandas.DataFrame): DataFrame with centrality measures.\n",
    "    \"\"\"\n",
    "    # Calculate PageRank centrality. This measure reflects the importance of a node in the graph.\n",
    "    # Nodes with a high PageRank centrality are those that are well connected or connected to well-connected nodes.\n",
    "    pr_w = pagerank(g, weight=g.ep.edge_weight).a\n",
    "\n",
    "    # Calculate betweenness centrality. This measure reflects the amount of control that a node exerts over the interactions of other nodes in the graph.\n",
    "    # Nodes with high betweenness centrality are those that lie on many shortest paths between other nodes.\n",
    "    bw_w = betweenness(g, weight=g.ep.edge_weight)[0].a\n",
    "\n",
    "    # Calculate closeness centrality. This measure reflects how close a node is to all other nodes in the graph.\n",
    "    # Nodes with high closeness centrality can reach other nodes quickly.\n",
    "    cl_w = closeness(g, weight=g.ep.edge_weight).a\n",
    "\n",
    "    # Calculate eigenvector centrality. This measure reflects the influence of a node in a network.\n",
    "    # Nodes with high eigenvector centrality are those connected to many nodes who themselves have high centrality.\n",
    "    ev_w = eigenvector(g, weight=g.ep.edge_weight)[1].a\n",
    "\n",
    "    # Calculate Katz centrality. This measure is a generalization of degree centrality and eigenvector centrality.\n",
    "    kt_w = katz(g, weight=g.ep.edge_weight).a\n",
    "\n",
    "    # Calculate HITS authority and hub scores. The HITS algorithm computes two numbers for a node: \n",
    "    # Authorities estimates the node value based on the incoming links. \n",
    "    # Hubs estimates the node value based on outgoing links.\n",
    "    hits_aut_w = hits(g, weight=g.ep.edge_weight)[1].a\n",
    "    hits_hub_w = hits(g, weight=g.ep.edge_weight)[2].a\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        \"pagerank\": pr_w,\n",
    "        \"betweenness\": bw_w,\n",
    "        \"closeness\": cl_w,\n",
    "        \"eigenvector\": ev_w,\n",
    "        \"katz\": kt_w,\n",
    "        \"authority\": hits_aut_w,\n",
    "        \"hub\": hits_hub_w,\n",
    "    })\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f128ecc8",
   "metadata": {},
   "source": [
    "## Understand adjacency and distance matrix\n",
    "A Gower's distance matrix and an adjacency matrix are both square matrices that represent relationships between entities, but they are used in different contexts and convey different types of information.\n",
    "\n",
    "1. **Gower's Distance Matrix**: This matrix is used in the context of cluster analysis or hierarchical clustering. Each entry in the matrix represents the Gower's distance between two entities (e.g., individuals, observations, etc.). Gower's distance is a measure of dissimilarity between two entities and takes into account both numerical and categorical variables. The smaller the Gower's distance, the more similar the two entities are.\n",
    "\n",
    "2. **Adjacency Matrix**: This matrix is used in the context of graph theory. Each entry in the matrix indicates whether two vertices (i.e., nodes) in a graph are adjacent to each other. In an unweighted graph, the entries are binary (0 or 1), indicating the absence or presence of an edge between two vertices. In a weighted graph, the entries can be any non-negative real number, representing the weight of the edge between two vertices.\n",
    "\n",
    "In some cases, a Gower's distance matrix could be transformed into a form of adjacency matrix. For example, you might create a graph where each entity is a node, and an edge exists between two nodes if their Gower's distance is below a certain threshold. The resulting adjacency matrix would represent a graph of entities that are similar to each other according to the Gower's distance. However, this would be a specific use case and not a general relationship between Gower's distance matrices and adjacency matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5e9d63",
   "metadata": {},
   "source": [
    "## Explanation\n",
    "1. **Adjacency Matrix**: An adjacency matrix is a square matrix used to represent a finite graph. In an unweighted graph, the elements of the adjacency matrix (A[i][j]) indicate whether pairs of vertices are adjacent or not in the graph -- 1 for adjacent, 0 for not adjacent. In a weighted graph, the elements of the adjacency matrix represent the weights of the edges.\n",
    "\n",
    "2. **Distance Matrix**: A distance matrix is a square matrix containing the distances, taken pairwise, between the elements of a set. In an unweighted graph, the elements of the distance matrix (D[i][j]) represent the number of edges in the shortest path between each pair of vertices. In a weighted graph, the elements of the distance matrix represent the sum of the weights of the shortest path between each pair of vertices.\n",
    "\n",
    "3. **Gower's Distance Matrix**: Gower's distance matrix is a special type of distance matrix primarily used for mixed data types. It is not commonly used in graph theory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce5c011",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.439Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load adjacency matrix\n",
    "# The adjacency matrix is a square matrix used to represent a finite graph.\n",
    "# The elements of the matrix indicate whether pairs of vertices are adjacent or not in the graph.\n",
    "adj_path = generate_file_path(\"data//Dist_matrix(\"+str(num_default_samples*2)+\").npy\")\n",
    "adj = np.load(adj_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a05fad",
   "metadata": {},
   "source": [
    "Explain: Here, this graph is a fully connected, which means each pair of nodes has an edge. Thus, the Adjacency Matrix is equivalent to the Gower's Distance Matrix.\n",
    "\n",
    "(When a graph is not fully connected, these two matrix are not equivalent any more.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0d4d5f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.441Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert adjacency matrix to graph\n",
    "# The adjacency matrix is then converted into a graph object using the `matrix_to_graph_tool` function.\n",
    "# This function takes the adjacency matrix as input and returns a graph object that can be manipulated using the `graph-tool` library.\n",
    "g = matrix_to_graph_tool(adj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048b53de",
   "metadata": {},
   "source": [
    "Attention here! We can use alpha threathold here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a808b8e4",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.442Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate minimum spanning tree and set as edge filter\n",
    "# A minimum spanning tree (MST) of a graph is a subset of the edges of the graph that connects all the vertices together,\n",
    "# without any cycles and with the minimum possible total edge weight.\n",
    "# The `min_spanning_tree` function from the `graph-tool` library is used to calculate the MST.\n",
    "# The resulting MST is then set as an edge filter on the graph using `g.set_edge_filter(tree)`.\n",
    "# This means that only the edges that are part of the MST will be considered in subsequent operations on the graph.\n",
    "tree = min_spanning_tree(g, weights=g.ep.edge_weight)\n",
    "g.set_edge_filter(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd2d11d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.443Z"
    }
   },
   "outputs": [],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a447ea8",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.443Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate centrality measures\n",
    "# Finally, several centrality measures are calculated for the graph using the `calculate_centrality_measures` function.\n",
    "# Centrality measures provide a way of identifying the most important vertices within a graph.\n",
    "# They are commonly used in network analysis to determine the relative importance of a vertex within the graph.\n",
    "# The resulting centrality measures are stored in a DataFrame.\n",
    "centrality_df = calculate_centrality_measures(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e5b39b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.445Z"
    }
   },
   "outputs": [],
   "source": [
    "centrality_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489a1c87",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.446Z"
    }
   },
   "outputs": [],
   "source": [
    "type(centrality_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc739f5f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.447Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "def plot_histogram(df, column):\n",
    "    \"\"\"\n",
    "    对于每列，绘制直方图。\n",
    "    如果这一列全是NaN，输出警告并跳过。\n",
    "    \"\"\"\n",
    "    if df[column].isna().all(): # 如果列全是NaN值，输出警告并跳过\n",
    "        print(f\"Warning: Column {column} only contains NaN values. Skipping...\")\n",
    "        return\n",
    "\n",
    "    plt.figure()\n",
    "    sns.histplot(df[column], kde=False, bins=50) # 限制bins数量为50\n",
    "    plt.title(f\"Histogram of {column}\")\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "centrality_data_path = generate_file_path(\"graph/Bondora_centrality_sample(\"+str(num_default_samples*2)+\").pdf\")\n",
    "\n",
    "with PdfPages(centrality_data_path) as pdf: # 输出至PDF\n",
    "    for column in centrality_df.columns:\n",
    "        plot_histogram(centrality_df, column)\n",
    "        if plt.get_fignums():  # 检查是否有活动的figure\n",
    "            pdf.savefig()  # 将当前figure保存到pdf中\n",
    "            plt.close()    # 关闭当前figure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971cd0c1",
   "metadata": {},
   "source": [
    "## Combine ans save files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3af8b54",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.448Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load original dataset\n",
    "sample_data_path = generate_file_path(\"data/Bondora_sample(\"+str(num_default_samples*2)+\").feather\")\n",
    "df_sample = pd.read_feather(sample_data_path)\n",
    "print(sample_data_path)\n",
    "\n",
    "# Merge with original dataset\n",
    "df_sample_with_centrality = pd.concat([df_sample, centrality_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8311e1a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.449Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df_sample_with_centrality\n",
    "\n",
    "# Define the text file path\n",
    "txt_file_path = generate_file_path(\"data//Bondora_sample(\"+str(num_default_samples*2)+\")_with_centrality.csv\")\n",
    "print(txt_file_path)\n",
    "# Write the DataFrame to the text file\n",
    "df.to_csv(txt_file_path, sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949d5d23",
   "metadata": {},
   "source": [
    "# Playground"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0bdbf8",
   "metadata": {},
   "source": [
    "## Graph, Nodes and Edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b43ac1",
   "metadata": {},
   "source": [
    "In graph theory, a graph is a collection of nodes (also known as vertices) and edges. The nodes represent entities or elements, while the edges represent the connections or relationships between these entities. \n",
    "\n",
    "**Nodes**: Nodes, also referred to as vertices, are the fundamental units in a graph. They represent individual elements or entities within a network. Nodes can represent various things depending on the context of the graph. For example, in a social network graph, nodes can represent individuals, while in a transportation network, nodes can represent cities or intersections.\n",
    "\n",
    "**Edges**: Edges are the connections or links between nodes in a graph. They represent the relationships or interactions between the entities represented by the nodes. Edges can be directed or undirected, depending on whether the relationship between nodes has a specific direction or not. Directed edges have an arrow indicating the direction of the relationship, while undirected edges have no specific direction. Edges in a graph represent relationships or interactions between the nodes. The nature of these edges can be very diverse depending on the context of your data and the problem you are solving. Here are a few examples of how edges can be defined:\n",
    "\n",
    "1. **Social Networks**: If your nodes are individuals, an edge could represent a friendship, a follow, or a connection on a social media platform.\n",
    "\n",
    "2. **Financial Networks**: If your nodes are banks, an edge could represent lending and borrowing relationships. In a financial transaction network, an edge could represent a transaction between two individuals.\n",
    "\n",
    "3. **Communication Networks**: If your nodes are individual email addresses, an edge could represent the exchange of emails between them.\n",
    "\n",
    "4. **Collaboration Networks**: If your nodes are scientists, an edge could represent co-authorship on a paper.\n",
    "\n",
    "5. **Infrastructure Networks**: If your nodes are cities, an edge could represent a direct road or flight connection between them.\n",
    "\n",
    "In your specific case, as you're dealing with a financial dataset (default/no default), the creation of edges depends on the context:\n",
    "\n",
    "- If your data includes transactions, you could create edges between entities that have transactions between them.\n",
    "- If your dataset includes information about co-signers or guarantors on loans, you could create edges between these connected individuals.\n",
    "- If your entities belong to the same group (like same organization or family), you could draw edges between them.\n",
    "- You could define edges based on similarity or proximity in the feature space. For instance, you could use a threshold value on a given feature or use a clustering method to group similar entities and then create edges within each group.\n",
    "\n",
    "In order to create a meaningful graph, it's crucial to have a clear understanding of your data and the relationships you're trying to represent. It's important to choose an edge definition that makes sense for your specific problem and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2641b7",
   "metadata": {},
   "source": [
    "## Centrality measures: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc22ece",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T17:43:49.925111Z",
     "start_time": "2023-05-21T17:43:49.905824Z"
    }
   },
   "source": [
    "Centrality measures are techniques used in network analysis to identify the most important nodes in a network. There are several types of centrality measures, such as degree centrality, closeness centrality, betweenness centrality, and eigenvector centrality. Here's a brief explanation of each:\n",
    "\n",
    "1. **Degree Centrality**: It is simply the number of edges connected to a node. In directed networks, we can differentiate between in-degree and out-degree centralities.\n",
    "\n",
    "   For an undirected graph, the degree centrality \\(C_D(v)\\) for a node \\(v\\) is calculated as:\n",
    "\n",
    "   <div style=\"text-align: center; font-weight: bold;\">\n",
    "   \\(C_D(v) = \\text{deg}(v)\\)\n",
    "   </div>\n",
    "\n",
    "   where \\(\\text{deg}(v)\\) is the degree of the node \\(v\\), i.e., the number of edges incident upon \\(v\\).\n",
    "\n",
    "   For a directed graph, we can define in-degree centrality and out-degree centrality. The in-degree centrality for a node \\(v\\) is the number of incoming edges to \\(v\\), and the out-degree centrality is the number of outgoing edges from \\(v\\).\n",
    "\n",
    "   Reference: Freeman, L. C. (2002). Centrality in social networks: Conceptual clarification. Social network: critical concepts in sociology. Londres: Routledge, 1, 238-263.\n",
    "\n",
    "2. **Closeness Centrality**: It measures how fast information can spread from a given node to other reachable nodes in the network.\n",
    "\n",
    "   <div style=\"text-align: center; font-weight: bold;\">\n",
    "   \\[C(x) = \\frac{1}{\\sum_{y}d(y, x)}\\]\n",
    "   </div>\n",
    "\n",
    "   In this formula, \\(d(y, x)\\) is the shortest-path distance from \\(y\\) to \\(x\\), and the sum is over all nodes \\(y\\) in the same connected component as \\(x\\). The idea is that the more central a node is, the closer it is to all other nodes.\n",
    "\n",
    "   Reference: Freeman, L. C. (2002). Centrality in social networks: Conceptual clarification. Social network: critical concepts in sociology. Londres: Routledge, 1, 238-263.\n",
    "\n",
    "3. **Betweenness Centrality**: It is a measure of a node's centrality in a network equal to the number of shortest paths from all vertices to all others that pass through that node.\n",
    "\n",
    "   The mathematical formula for betweenness centrality \\(C_B(v)\\) for a node \\(v\\) is:\n",
    "\n",
    "   <div style=\"text-align: center; font-weight: bold;\">\n",
    "   \\[C_B(v) =\\sum_{s,t \\in V} \\frac{\\sigma(s, t|v)}{\\sigma(s, t)}\\]\n",
    "   </div>\n",
    "\n",
    "   In this formula, \\(V\\) is the set of nodes, \\(\\sigma(s, t)\\) is the total number of shortest paths from node \\(s\\) to node \\(t\\), and \\(\\sigma(s, t|v)\\) is the number of those paths that pass through \\(v\\).\n",
    "\n",
    "   Reference: Freeman, L. C. (1977). A set of measures of centrality based on betweenness. Sociometry, 35-41.\n",
    "\n",
    "4. **Eigenvector Centrality**: A node is considered important if it is connected to other important nodes.\n",
    "\n",
    "   The mathematical formula for eigenvector centrality \\(C_E(v)\\) for a node \\(v\\) is defined as:\n",
    "\n",
    "   <div style=\"text-align: center; font-weight: bold;\">\n",
    "   \\[CE(v) = \\frac{1}{\\lambda} \\sum_{t \\in M(v)} C_E(t)\\]\n",
    "   </div>\n",
    "\n",
    "   In this formula, \\(M(v)\\) is the set of the neighbors of \\(v\\), and \\(\\lambda\\) is a constant. In other words, the eigenvector centrality for a node is the sum of the centrality scores of its neighbors, scaled by a constant factor.\n",
    "\n",
    "   The calculation of eigenvector centrality is indeed iterative and based on the centrality of the neighboring nodes. This is because the importance of a node in the network is determined not only by how many connections it has, but also by how important its connections are.\n",
    "\n",
    "   The calculation of eigenvector centrality is typically done through the power iteration method. Here's a simplified explanation of the process:\n",
    "\n",
    "   1. Assign all nodes an initial centrality score. This could be a score of 1 for simplicity.\n",
    "\n",
    "   2. For each node, calculate its new centrality score as the sum of the centrality scores of its neighbors from the previous iteration.\n",
    "\n",
    "   3. Normalize the centrality scores so that their sum is 1. This is to prevent the scores from growing or shrinking exponentially in the next iterations.\n",
    "\n",
    "   4. Repeat steps 2 and 3 until the scores converge, i.e., the scores do not change significantly from one iteration to the next.\n",
    "\n",
    "   The result is a vector of centrality scores that is the principal eigenvector of the adjacency matrix of the graph, hence the name \"eigenvector centrality\".\n",
    "\n",
    "   Eigenvector centrality is a measure of the influence of a node in a network. It assigns relative scores to all nodes in the network based on the concept that connections to high-scoring nodes contribute more to the score of the node in question than equal connections to low-scoring nodes.\n",
    "\n",
    "   Reference: Bonacich, P. (1987). Power and centrality: A family of measures. American journal of sociology, 92(5), 1170-1182.\n",
    "\n",
    "5. **PageRank**: is an algorithm used by Google Search to rank web pages in their search engine results. It works by counting the number and quality of links to a page to determine a rough estimate of how important the website is. The underlying assumption is that more important websites are likely to receive more links from other websites. The mathematical formula for PageRank \\(PR(p)\\) for a page \\(p\\) is:\n",
    "\n",
    "   <div style=\"text-align: center; font-weight: bold;\">\n",
    "   \\[PR(p) = (1 - d) + d \\sum_{i \\in M(p)} \\frac{PR(i)}{L(i)}\\]\n",
    "   </div>\n",
    "\n",
    "   In this formula, \\(M(p)\\) is the set of pages that link to \\(p\\), \\(L(i)\\) is the number of outbound links on page \\(i\\), and \\(d\\) is a damping factor, usually set to 0.85. The idea is that a page's PageRank is derived from the PageRanks of the pages that link to it. Each of these contributing pages transfers its PageRank to \\(p\\) proportionally to the number of outbound links it has.\n",
    "\n",
    "   Reference: Lawrence, P. (1999). The pagerank citation ranking: Bringing order to the web. [Link](http://dbpubs.stanford.edu:8090/aux/index-en.html)\n",
    "\n",
    "6. **Katz centrality**: Katz centrality is a measure of centrality in a network that takes into account both the direct and indirect influence of a node's neighbors. It was introduced by Leo Katz in his paper \"A New Status Index Derived from    Sociometric Analysis\" published in 1953.\n",
    "\n",
    "   The Katz centrality of a node \\(i\\) is defined as the sum of the contributions from all its neighbors, weighted by a factor \\(\\beta\\) and the number of paths of length \\(k\\) connecting the neighbors to the node \\(i\\). The formula for Katz centrality can be expressed as:\n",
    "\n",
    "   <div style=\"text-align: center; font-weight: bold;\">\n",
    "   \\(C_{\\text{Katz}}(i) = \\sum_{j=1}^{n} \\beta A_{ij} C_{\\text{Katz}}(j) + \\alpha\\)\n",
    "   </div>\n",
    "\n",
    "   where:\n",
    "   - \\(C_{\\text{Katz}}(i)\\) represents the Katz centrality of node \\(i\\).\n",
    "   - \\(A_{ij}\\) denotes the adjacency matrix element, indicating the presence or absence of an edge between nodes \\(i\\) and \\(j\\).\n",
    "   - \\(\\beta\\) is a scaling factor that controls the weight given to indirect paths. Typically, \\(|\\beta| < \\frac{1}{\\lambda_{\\text{max}}}\\), where \\(\\lambda_{\\text{max}}\\) is the largest eigenvalue of the adjacency matrix.\n",
    "   - \\(\\alpha\\) is a constant term representing the node's intrinsic centrality.\n",
    "\n",
    "   The Katz centrality algorithm is iterative. Starting with an initial centrality value for each node, the centrality values are updated iteratively using the above formula until convergence is reached.\n",
    "\n",
    "   Katz centrality is useful for identifying influential nodes in a network, as it considers both direct and indirect connections. Nodes with higher Katz centrality scores are considered more central or influential within the network.\n",
    "\n",
    "   Reference: Katz, L. (1953). A new status index derived from sociometric analysis. Psychometrika, 18(1), 39-43.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "6. **HITS Algorithm**: The HITS (Hyperlink-Induced Topic Search) algorithm, also known as hubs and authorities, is a link analysis algorithm that rates webpages, developed by Jon Kleinberg. The idea behind Hubs and Authorities stemmed from a particular insight into the creation of web pages when the Internet was originally forming; that is, certain web pages, known as hubs, served as large directories that were not actually authoritative in the information that they held, but were used as compilations of a broad catalog of information that led users to direct to other authoritative pages.\n",
    "\n",
    "   In the context of HITS, each node in a graph has two scores: an authority score and a hub score.\n",
    "\n",
    "   The Authority Score a(i) of a node i is computed as the sum of the hub scores of each node j that points to i. This can be represented mathematically as:\n",
    "\n",
    "   <div style=\"text-align: center; font-weight: bold;\">\n",
    "   \\[\n",
    "   a(i) = \\sum_{j \\in M(i)} h(j)\n",
    "   \\]\n",
    "   </div>\n",
    "\n",
    "   where: M(i) is the set of nodes that point to i and h(j) is the hub score of node j.\n",
    "   The Hub Score h(i) of a node i is computed as the sum of the authority scores of each node j that i points to. This can be represented mathematically as:\n",
    "   \n",
    "   <div style=\"text-align: center; font-weight: bold;\">\n",
    "   \\[\n",
    "   h(i) = \\sum_{j \\in N(i)} a(j)\n",
    "   \\]\n",
    "   </div>\n",
    "\n",
    "   where N(i) is the set of nodes that i points to and a(j) is the authority score of node j.\n",
    "\n",
    "   The authority and hub scores are calculated iteratively until they converge.\n",
    "\n",
    "   The HITS algorithm was first proposed by Jon Kleinberg in his work:\n",
    "\n",
    "   Reference: Kleinberg, J. M. (1999). Authoritative sources in a hyperlinked environment. Journal of the ACM (JACM), 46(5), 604-632."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cc3f68",
   "metadata": {},
   "source": [
    "## Application in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f49957",
   "metadata": {},
   "source": [
    "Now, if you're looking to compute centrality measures using features as inputs, you could consider the features as attributes of the nodes or edges in your network. For instance, if you're predicting default/non-default, each node could be an individual (or entity), and the features could be attributes of those individuals. The edges could represent some relationship between them.\n",
    "\n",
    "However, the computation of centrality doesn't typically involve the use of predictive features as inputs, but rather the structure of the network itself (who is connected to whom). If you have some prediction task related to the nodes of the network, the centrality measures can be used as features to help predict that task.\n",
    "\n",
    "For example, if you are trying to predict default/non-default for individuals in a financial network, you could calculate the centrality measures for each individual (node) in the network. These centrality measures could then serve as input features to a machine learning model (along with any other features you have about the individuals) to predict default/non-default.\n",
    "\n",
    "Here is a general way you might approach this using Python's NetworkX library:\n",
    "\n",
    "```python\n",
    "import networkx as nx\n",
    "\n",
    "# Create a graph object\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes and edges to your graph using your data\n",
    "# You can also add attributes to nodes and edges if you have additional features\n",
    "# For example:\n",
    "# G.add_node(node_id, attr_dict={feature1:value1, feature2:value2,...})\n",
    "# G.add_edge(node1_id, node2_id, attr_dict={feature1:value1, feature2:value2,...})\n",
    "\n",
    "# Calculate centrality measures\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "closeness_centrality = nx.closeness_centrality(G)\n",
    "betweenness_centrality = nx.betweenness_centrality(G)\n",
    "eigenvector_centrality = nx.eigenvector_centrality(G)\n",
    "page_rank = nx.pagerank(G, alpha=0.85)  # alpha is the damping parameter\n",
    "katz_centrality = nx.katz_centrality(G, alpha=0.1, beta=1.0)\n",
    "hubs, authorities = nx.hits(G)\n",
    "\n",
    "# You can then add these as node attributes (which could serve as additional features for your prediction task)\n",
    "for node_id in G.nodes():\n",
    "    G.nodes[node_id]['degree_centrality'] = degree_centrality[node_id]\n",
    "    G.nodes[node_id]['closeness_centrality'] = closeness_centrality[node_id]\n",
    "    G.nodes[node_id]['betweenness_centrality'] = betweenness_centrality[node_id]\n",
    "    G.nodes[node_id]['eigenvector_centrality'] = eigenvector_centrality[node_id]\n",
    "```\n",
    "\n",
    "These features can then be fed into a machine learning model to predict default/non-default. The exact method of doing this will depend on the specific machine learning model you are using."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee18b61",
   "metadata": {},
   "source": [
    "## Edges based on feature similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134c0c61",
   "metadata": {},
   "source": [
    "Defining edges based on similarity or proximity in the feature space is a common technique used in the field of network science. Here is a basic example of how to create edges based on Euclidean distance, a common measure of proximity in the feature space:\n",
    "\n",
    "```python\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Create a synthetic dataset\n",
    "X, y = make_classification(n_samples=100, n_features=5, n_informative=2, n_redundant=0, random_state=1)\n",
    "\n",
    "# Convert feature matrix into a DataFrame\n",
    "df = pd.DataFrame(X, columns=['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Feature5'])\n",
    "df['Default'] = y\n",
    "\n",
    "# Create a graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes to the graph\n",
    "for index, row in df.iterrows():\n",
    "    G.add_node(index, attr_dict=row.to_dict())\n",
    "\n",
    "# Compute pairwise Euclidean distances\n",
    "distances = euclidean_distances(df.values)\n",
    "\n",
    "# Define a threshold to decide if an edge should be added\n",
    "threshold = distances.mean()\n",
    "\n",
    "# Add edges between nodes that are closer than the threshold distance\n",
    "for i in range(distances.shape[0]):\n",
    "    for j in range(i+1, distances.shape[0]):  # we only need to look at half the matrix\n",
    "        if distances[i, j] < threshold:\n",
    "            G.add_edge(i, j)\n",
    "```\n",
    "\n",
    "This will create a graph where each node represents an individual, and an edge exists between any two individuals if the Euclidean distance between their feature vectors is less than the average Euclidean distance across all pairs of individuals.\n",
    "\n",
    "Remember that the choice of distance metric (Euclidean in this case) and threshold is crucial and can greatly affect the resulting graph. You may want to experiment with different distance metrics (e.g., Manhattan, Cosine, etc.) and thresholds to find what works best for your specific case.\n",
    "\n",
    "Also, this approach does not scale well for large datasets due to the computation of the pairwise distance matrix. For large datasets, consider using more scalable methods like Nearest Neighbors or approximate methods for large-scale similarity computations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eec3e5f",
   "metadata": {},
   "source": [
    "## Edges for a weighted graph based on feature similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8127e2",
   "metadata": {},
   "source": [
    "If you want to use a weighted graph, you can add weights to the edges that correspond to the similarity or proximity in the feature space. In the context of the previous example, you could use the inverse of the Euclidean distance as the edge weight. This means that nodes that are more similar (smaller distance) will have a stronger connection (larger weight).\n",
    "\n",
    "Here's how you would modify the previous example to create a weighted graph:\n",
    "\n",
    "```python\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Create a synthetic dataset\n",
    "X, y = make_classification(n_samples=100, n_features=5, n_informative=2, n_redundant=0, random_state=1)\n",
    "\n",
    "# Convert feature matrix into a DataFrame\n",
    "df = pd.DataFrame(X, columns=['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Feature5'])\n",
    "df['Default'] = y\n",
    "\n",
    "# Create a graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes to the graph\n",
    "for index, row in df.iterrows():\n",
    "    G.add_node(index, attr_dict=row.to_dict())\n",
    "\n",
    "# Compute pairwise Euclidean distances\n",
    "distances = euclidean_distances(df.values)\n",
    "\n",
    "# Define a threshold to decide if an edge should be added\n",
    "threshold = distances.mean()\n",
    "\n",
    "# Add edges between nodes that are closer than the threshold distance\n",
    "# Use the inverse distance as the edge weight\n",
    "for i in range(distances.shape[0]):\n",
    "    for j in range(i+1, distances.shape[0]):  # we only need to look at half the matrix\n",
    "        if distances[i, j] < threshold:\n",
    "            weight = 1.0 / distances[i, j]  # inverse distance as weight\n",
    "            G.add_edge(i, j, weight=weight)\n",
    "```\n",
    "\n",
    "In this example, the edge weight is the inverse of the Euclidean distance, so nodes that are closer in the feature space have a higher weight. You can use any function of the distance as the weight depending on your specific requirements. Be careful with the possibility of division by zero when using the inverse distance as weight, you may want to add a small constant in the denominator to avoid this situation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a496c0f",
   "metadata": {},
   "source": [
    "## From edge weight to centrality measures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb7475b",
   "metadata": {},
   "source": [
    "Now that we have a graph, we can compute several centrality measures. Here's how you can compute degree, closeness, betweenness, and eigenvector centrality for the weighted graph. Note that the interpretation of these measures changes slightly when dealing with weighted graphs.\n",
    "\n",
    "```python\n",
    "# Compute centrality measures\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "closeness_centrality = nx.closeness_centrality(G, distance='weight')\n",
    "betweenness_centrality = nx.betweenness_centrality(G, weight='weight')\n",
    "eigenvector_centrality = nx.eigenvector_centrality(G, weight='weight')\n",
    "\n",
    "# Add these as node attributes\n",
    "for node_id in G.nodes():\n",
    "    G.nodes[node_id]['degree_centrality'] = degree_centrality[node_id]\n",
    "    G.nodes[node_id]['closeness_centrality'] = closeness_centrality[node_id]\n",
    "    G.nodes[node_id]['betweenness_centrality'] = betweenness_centrality[node_id]\n",
    "    G.nodes[node_id]['eigenvector_centrality'] = eigenvector_centrality[node_id]\n",
    "```\n",
    "\n",
    "Here's a brief explanation of each of these measures:\n",
    "\n",
    "1. **Degree Centrality**: In a weighted graph, degree centrality is still calculated as the number of edges connected to a node. This means that nodes with more connections will have a higher degree centrality. However, it doesn't take into account the weight of the edges, meaning that the strength of the connections is not considered in this measure.\n",
    "\n",
    "2. **Closeness Centrality**: This measure calculates the reciprocal of the sum of the shortest path distances from a node to all other nodes in the graph. So, a higher value of closeness centrality implies that a node is more central. In a weighted graph, the 'distances' considered are the weights of the edges, so a node that has stronger connections to all other nodes (i.e., higher edge weights) will have higher closeness centrality.\n",
    "\n",
    "3. **Betweenness Centrality**: This is a measure of the extent to which a node lies on paths between other nodes. Nodes with high betweenness centrality have a large influence on the transfer of items through the network, under the assumption that item transfer follows the shortest paths. When weights are considered, shorter paths are those with higher total weights, so nodes that lie on the paths with the strongest connections will have higher betweenness centrality.\n",
    "\n",
    "4. **Eigenvector Centrality**: This is a measure of the influence of a node in a network. It assigns relative scores to all nodes in the network based on the principle that connections to nodes with high score contribute more to the score of the node in question. In a weighted graph, connections to nodes with high scores and high edge weights contribute more to the score of the node.\n",
    "\n",
    "Remember that each of these measures captures a different aspect of a node's centrality, and the appropriate measure to use depends on the specifics of your problem and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65635f78",
   "metadata": {},
   "source": [
    "## Centrality for binary targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e1cd2f",
   "metadata": {},
   "source": [
    "With your data, which includes a binary target (default or no default) and many numerical features, you can explore several analytical routes.\n",
    "\n",
    "The centrality measures we've computed can be used to examine the relationships between the nodes (which represent your entities - individuals or organizations) in the context of the target variable (default or no default). For instance, you might find that entities with high degree centrality are more (or less) likely to default, indicating a potential relationship between the entity's position in the network and their default risk.\n",
    "\n",
    "Here's how you can explore these relationships:\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert node attributes to a DataFrame\n",
    "df_node_attributes = pd.DataFrame(dict(G.nodes(data=True))).T\n",
    "\n",
    "# Plot centrality measures against the default status\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 12))\n",
    "\n",
    "# Degree centrality\n",
    "axs[0, 0].scatter(df_node_attributes['Default'], df_node_attributes['degree_centrality'])\n",
    "axs[0, 0].set_xlabel('Default')\n",
    "axs[0, 0].set_ylabel('Degree Centrality')\n",
    "\n",
    "# Closeness centrality\n",
    "axs[0, 1].scatter(df_node_attributes['Default'], df_node_attributes['closeness_centrality'])\n",
    "axs[0, 1].set_xlabel('Default')\n",
    "axs[0, 1].set_ylabel('Closeness Centrality')\n",
    "\n",
    "# Betweenness centrality\n",
    "axs[1, 0].scatter(df_node_attributes['Default'], df_node_attributes['betweenness_centrality'])\n",
    "axs[1, 0].set_xlabel('Default')\n",
    "axs[1, 0].set_ylabel('Betweenness Centrality')\n",
    "\n",
    "# Eigenvector centrality\n",
    "axs[1, 1].scatter(df_node_attributes['Default'], df_node_attributes['eigenvector_centrality'])\n",
    "axs[1, 1].set_xlabel('Default')\n",
    "axs[1, 1].set_ylabel('Eigenvector Centrality')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "You can also calculate correlations between these centrality measures and the target variable to see if there are any strong relationships.\n",
    "\n",
    "```python\n",
    "# Calculate correlations\n",
    "correlations = df_node_attributes[['Default', 'degree_centrality', 'closeness_centrality', 'betweenness_centrality', 'eigenvector_centrality']].astype(float).corr()\n",
    "\n",
    "# Display correlations with the target variable\n",
    "print(correlations['Default'])\n",
    "```\n",
    "\n",
    "It's important to note that this is exploratory analysis and correlation does not imply causation. Further investigation would be needed to understand any potential causal relationships.\n",
    "\n",
    "In the long run, these network-based features can also be used in conjunction with your other numerical features to build a predictive model. By including the network features, the model may be able to capture complex relationships that are not captured by the numerical features alone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3eb633",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8fa2bd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Create data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8ced57",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.453Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Create a synthetic dataset\n",
    "X, y = make_classification(n_samples=2000, n_features=5, n_informative=2, n_redundant=0, random_state=1)\n",
    "\n",
    "# Automatically name the columns based on the number of features\n",
    "column_names = [f\"Feature{i}\" for i in range(1, X.shape[1])]\n",
    "column_names.append(\"Default\")\n",
    "# Convert feature matrix into a DataFrame\n",
    "df = pd.DataFrame(X, columns=column_names)\n",
    "\n",
    "df['Default'] = y\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50728307",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Scale features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b412c75",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.454Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the features (excluding the 'Default' column)\n",
    "scaler.fit(df.drop('Default', axis=1))\n",
    "\n",
    "# Transform the features\n",
    "scaled_features = scaler.transform(df.drop('Default', axis=1))\n",
    "\n",
    "# Create a new DataFrame for the scaled features\n",
    "scaled_df = pd.DataFrame(scaled_features, columns=column_names[:-1])  # Exclude the 'Default' column name\n",
    "\n",
    "# Add the 'Default' column back into the DataFrame\n",
    "scaled_df['Default'] = df['Default']\n",
    "\n",
    "print(scaled_df)\n",
    "df = scaled_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7983449",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.455Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "def plot_feature_distributions(df_scaled, target_col, file_name = 'feature_distributions.pdf'):\n",
    "    with PdfPages(file_name) as pdf:\n",
    "        # For each feature\n",
    "        for col in df_scaled.columns:\n",
    "            # Exclude the target column\n",
    "            if col != target_col:\n",
    "                plt.figure(figsize=(10, 5))\n",
    "                \n",
    "                # Plot the distribution of this feature for each class\n",
    "                sns.kdeplot(data=df_scaled, x=col, hue=target_col, fill=True)\n",
    "                \n",
    "                plt.title(f'Distribution of {col} by Class')\n",
    "                \n",
    "                # Save the current figure to the pdf\n",
    "                pdf.savefig()\n",
    "                plt.clf()\n",
    "\n",
    "\n",
    "# Call the function\n",
    "plot_feature_distributions(scaled_df, 'Default')\n",
    "import webbrowser\n",
    "\n",
    "webbrowser.open_new(r'feature_distributions.pdf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60b8489",
   "metadata": {},
   "source": [
    "### Remove highly correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63e37a7",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.456Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def remove_highly_correlated_features(df, threshold=0.8):\n",
    "    \"\"\"\n",
    "    Remove highly correlated features from a DataFrame based on a given threshold.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The input DataFrame.\n",
    "    threshold (float): The threshold for high correlation. Default is 0.8.\n",
    "\n",
    "    Returns:\n",
    "    df_filtered (pandas.DataFrame): The filtered DataFrame with highly correlated features removed.\n",
    "    \"\"\"\n",
    "\n",
    "    features_to_remove = []\n",
    "\n",
    "    # Exclude the \"Default\" column from correlation analysis\n",
    "    df_subset = df.drop(\"Default\", axis=1)\n",
    "\n",
    "    # Calculate the correlation matrix\n",
    "    correlation_matrix = df_subset.corr()\n",
    "\n",
    "    # Identify highly correlated features\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            if abs(correlation_matrix.iloc[i, j]) > threshold:\n",
    "                # Append the feature to the removal list\n",
    "                features_to_remove.append(correlation_matrix.columns[j])\n",
    "\n",
    "    # Remove the highly correlated features\n",
    "    df_filtered = df.drop(features_to_remove, axis=1)\n",
    "\n",
    "    return df_filtered, correlation_matrix, features_to_remove\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1b8c95",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.457Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove highly correlated features\n",
    "df, correlation_matrix,features_to_remove = remove_highly_correlated_features(df, threshold=0.9)\n",
    "print(correlation_matrix)\n",
    "# Print the filtered DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b776b018",
   "metadata": {},
   "source": [
    "### Create Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be969549",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.458Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a graph\n",
    "G = nx.Graph()\n",
    "\n",
    "\n",
    "# Add nodes to the graph\n",
    "for index, row in df.iterrows():\n",
    "    G.add_node(index, attr_dict=row.to_dict())\n",
    "\n",
    "# Compute pairwise Euclidean distances\n",
    "distances = euclidean_distances(df.values)\n",
    "\n",
    "# Define a threshold to decide if an edge should be added\n",
    "threshold = distances.mean()\n",
    "\n",
    "# Add edges between nodes that are closer than the threshold distance\n",
    "# Use the inverse distance as the edge weight\n",
    "for i in range(distances.shape[0]):\n",
    "    for j in range(i+1, distances.shape[0]):  # we only need to look at half the matrix\n",
    "        if distances[i, j] < threshold:\n",
    "            weight = 1.0 / distances[i, j]  # inverse distance as weight\n",
    "            G.add_edge(i, j, weight=weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7134d1d",
   "metadata": {},
   "source": [
    "### Visualize graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69501dc3",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.459Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate positions for all nodes in the graph G\n",
    "pos = nx.spring_layout(G)\n",
    "\n",
    "# Create a color map\n",
    "color_map = []\n",
    "for node in G:\n",
    "    if G.nodes[node]['attr_dict']['Default'] == 1:\n",
    "        color_map.append('red')\n",
    "    else: \n",
    "        color_map.append('blue')\n",
    "\n",
    "# Draw the graph with node colors\n",
    "nx.draw_networkx_nodes(G, pos, node_color=color_map)\n",
    "nx.draw_networkx_edges(G, pos, edge_color='grey')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308fb308",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.460Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We need pygraphviz for that.\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pygraphviz as pgv\n",
    "from networkx.drawing.nx_agraph import graphviz_layout\n",
    "\n",
    "# Convert the NetworkX graph G to a PyGraphviz graph\n",
    "A = nx.nx_agraph.to_agraph(G)\n",
    "\n",
    "# Create a layout for the graph\n",
    "pos = graphviz_layout(G, prog='dot')\n",
    "\n",
    "# Draw nodes with color coding for the 'Default' attribute\n",
    "nx.draw_networkx_nodes(G, pos, node_color=color_map)\n",
    "nx.draw_networkx_edges(G, pos, edge_color='grey')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36abd888",
   "metadata": {},
   "source": [
    "### Visualize data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f99053",
   "metadata": {},
   "source": [
    "There are several ways you could visualize this dataset. Because it has 5 numerical features, you could use pairwise scatterplots to visualize the relationship between pairs of features. You could also use histograms or boxplots to visualize the distribution of each feature. Here are examples of how to create these visualizations using matplotlib and seaborn:\n",
    "\n",
    "1. **Pairwise Scatterplots:**\n",
    "\n",
    "```python\n",
    "import seaborn as sns\n",
    "\n",
    "# Pairplot of the dataset\n",
    "sns.pairplot(df, hue='Default')\n",
    "```\n",
    "The above code will generate a pairwise scatterplot matrix. Each plot represents the relationship between two features, and data points are colored based on their 'Default' status. The diagonal line of the matrix shows the distribution of the single feature according to the 'Default' categories.\n",
    "\n",
    "2. **Boxplots:**\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Boxplots for each feature\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "for i, column in enumerate(df.columns[:-1]):  # excluding the 'Default' column\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    sns.boxplot(x='Default', y=column, data=df)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "The boxplots visualize the distribution of each feature for each 'Default' status separately. You can observe the median, quartiles and possible outliers for each feature in each 'Default' category.\n",
    "\n",
    "Remember, these are just simple ways to visualize your dataset and there are many other visualization techniques that can provide deeper insights depending on the nature of your data. For example, a correlation heatmap could be useful to identify highly correlated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b641a3b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.462Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Pairplot of the dataset\n",
    "sns.pairplot(df, hue='Default')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88132f6",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.463Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def create_boxplots(df):\n",
    "    \"\"\"\n",
    "    Create boxplots for each feature in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Determine the number of features and calculate the appropriate number of subplots\n",
    "    num_features = len(df.columns) - 1  # excluding the 'Default' column\n",
    "    num_rows = (num_features - 1) // 3 + 1\n",
    "    num_cols = min(num_features, 3)\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, 10))\n",
    "\n",
    "    # Flatten the axes array if needed\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Plot boxplots for each feature\n",
    "    for i, column in enumerate(df.columns[:-1]):  # excluding the 'Default' column\n",
    "        ax = axes[i]\n",
    "        sns.boxplot(x='Default', y=column, data=df, ax=ax)\n",
    "\n",
    "    # Remove any unused subplots\n",
    "    if num_features < len(axes):\n",
    "        for j in range(num_features, len(axes)):\n",
    "            axes[j].remove()\n",
    "\n",
    "    # Adjust the layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to create boxplots for each feature\n",
    "create_boxplots(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534936ed",
   "metadata": {},
   "source": [
    "### Compute minimum spanning tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26e7dbb",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.464Z"
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "mst = nx.minimum_spanning_tree(G)\n",
    "import networkx as nx\n",
    "\n",
    "# Create an edge filter for the MST\n",
    "def edge_filter(u, v):\n",
    "    return (u, v) in mst.edges()\n",
    "\n",
    "# Create a filtered subgraph based on the edge filter\n",
    "filtered_graph = G.edge_subgraph((u, v) for u, v in G.edges() if edge_filter(u, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf6d5d8",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.465Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the filtered subgraph\n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw(G, pos, with_labels=True)\n",
    "labels = nx.get_edge_attributes(G, 'weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39ed2e7",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.466Z"
    }
   },
   "outputs": [],
   "source": [
    "nx.draw_networkx_edges(filtered_graph, pos, edge_color='r', width=2)  # Highlight filtered edges in red\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042e3348",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.467Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate positions for all nodes in the graph G\n",
    "pos = nx.spring_layout(G)\n",
    "\n",
    "# Create a color map\n",
    "color_map = []\n",
    "for node in G:\n",
    "    if G.nodes[node]['attr_dict']['Default'] == 1:\n",
    "        color_map.append('red')\n",
    "    else: \n",
    "        color_map.append('blue')\n",
    "\n",
    "# Draw the graph with node colors\n",
    "nx.draw_networkx_nodes(G, pos, node_color=color_map)\n",
    "nx.draw_networkx_edges(G, pos, edge_color='grey')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3458f7fb",
   "metadata": {},
   "source": [
    "### Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836ef9e9",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.468Z"
    }
   },
   "outputs": [],
   "source": [
    "nx.betweenness_centrality?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6036f1c5",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.469Z"
    }
   },
   "outputs": [],
   "source": [
    "G = filtered_graph\n",
    "# Compute centrality measures\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "closeness_centrality = nx.closeness_centrality(G, distance='weight')\n",
    "betweenness_centrality = nx.betweenness_centrality(G, weight='weight')\n",
    "eigenvector_centrality = nx.eigenvector_centrality(G, weight='weight')\n",
    "\n",
    "   \n",
    "    \n",
    "# Compute more centrality measures\n",
    "pagerank = nx.pagerank(G, weight='weight')\n",
    "# HITS algorithm returns two dictionaries keyed by node containing hub scores and authority scores\n",
    "hubs, authorities = nx.hits(G, max_iter=1000)\n",
    "\n",
    "# Compute other network measures\n",
    "#avg_shortest_path_length = nx.average_shortest_path_length(G)\n",
    "#density = nx.density(G)\n",
    "#num_connected_components = nx.number_connected_components(G)\n",
    "#avg_clustering_coefficient = nx.average_clustering(G)\n",
    "\n",
    "# Add these as node attributes\n",
    "for node_id in G.nodes():\n",
    "    G.nodes[node_id]['degree_centrality'] = degree_centrality[node_id]\n",
    "    G.nodes[node_id]['closeness_centrality'] = closeness_centrality[node_id]\n",
    "    G.nodes[node_id]['betweenness_centrality'] = betweenness_centrality[node_id]\n",
    "    G.nodes[node_id]['eigenvector_centrality'] = eigenvector_centrality[node_id]\n",
    "\n",
    "    G.nodes[node_id]['pagerank'] = pagerank[node_id]\n",
    "    G.nodes[node_id]['hub_score'] = hubs[node_id]\n",
    "    G.nodes[node_id]['authority_score'] = authorities[node_id]\n",
    "#    G.nodes[node_id]['average_shortest_path_length'] = avg_shortest_path_length\n",
    "#    G.nodes[node_id]['density'] = density\n",
    "#    G.nodes[node_id]['num_connected_components'] = num_connected_components\n",
    "#    G.nodes[node_id]['average_clustering_coefficient'] = avg_clustering_coefficient\n",
    "\n",
    "# Note: Some measures like average shortest path length, density, number of connected components, \n",
    "# and average clustering coefficient are properties of the network as a whole rather than individual nodes. \n",
    "# Therefore, they will be the same for all nodes in a connected graph. \n",
    "# You might want to use these measures for comparison across different graphs or subgraphs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471c38d0",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.470Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert node attributes to a DataFrame and flatten the column structure\n",
    "df_node_attributes = pd.json_normalize(list(dict(G.nodes(data=True)).values()))\n",
    "\n",
    "# Calculate correlations\n",
    "correlations = df_node_attributes.astype(float).corr()\n",
    "\n",
    "# Display correlations with the target variable sorted in descending order\n",
    "print(correlations['attr_dict.Default'].sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e61726",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.471Z"
    }
   },
   "outputs": [],
   "source": [
    "correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe77adbe",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.472Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Convert node attributes to a DataFrame and flatten the column structure\n",
    "df_node_attributes = pd.json_normalize(list(dict(G.nodes(data=True)).values()))\n",
    "\n",
    "# Calculate correlations\n",
    "correlations = df_node_attributes.astype(float).corr()\n",
    "\n",
    "# Use a mask to get the lower triangle as the correlation matrix is symmetric\n",
    "mask = np.triu(np.ones_like(correlations, dtype=bool))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(correlations, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a30a23",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.473Z"
    }
   },
   "outputs": [],
   "source": [
    "# Call the function\n",
    "import webbrowser\n",
    "plot_feature_distributions(\n",
    "    df_node_attributes,\n",
    "    'attr_dict.Default',\n",
    "    file_name=\"feature_distributions_with_centrality.pdf\")\n",
    "\n",
    "webbrowser.open_new(r'feature_distributions_with_centrality.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65b3090",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.474Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_centrality_measures(df_node_attributes):\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 12))\n",
    "\n",
    "    # Degree centrality\n",
    "    axs[0, 0].scatter(df_node_attributes['attr_dict.Default'], df_node_attributes['degree_centrality'])\n",
    "    axs[0, 0].set_xlabel('attr_dict.Default')\n",
    "    axs[0, 0].set_ylabel('Degree Centrality')\n",
    "\n",
    "    # Closeness centrality\n",
    "    axs[0, 1].scatter(df_node_attributes['attr_dict.Default'], df_node_attributes['closeness_centrality'])\n",
    "    axs[0, 1].set_xlabel('attr_dict.Default')\n",
    "    axs[0, 1].set_ylabel('Closeness Centrality')\n",
    "\n",
    "    # Betweenness centrality\n",
    "    axs[1, 0].scatter(df_node_attributes['attr_dict.Default'], df_node_attributes['betweenness_centrality'])\n",
    "    axs[1, 0].set_xlabel('attr_dict.Default')\n",
    "    axs[1, 0].set_ylabel('Betweenness Centrality')\n",
    "\n",
    "    # Eigenvector centrality\n",
    "    axs[1, 1].scatter(df_node_attributes['attr_dict.Default'], df_node_attributes['eigenvector_centrality'])\n",
    "    axs[1, 1].set_xlabel('attr_dict.Default')\n",
    "    axs[1, 1].set_ylabel('Eigenvector Centrality')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ce22cc",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.476Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_centrality_measures(df_node_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89da1f6b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.477Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 将Jupyter notebook转换为幻灯片\n",
    "os.system(\"jupyter nbconvert 0.1_data_preprocessing.ipynb --to slides\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719cd70b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.478Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 将HTML幻灯片转换为PDF\n",
    "os.system(\"pandoc 0.1_data_preprocessing.slides.html -t beamer -o your_notebook.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85438d6a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.479Z"
    }
   },
   "outputs": [],
   "source": [
    "!brew install pandoc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a76869",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T20:28:43.189351Z",
     "start_time": "2023-05-19T20:28:42.900506Z"
    }
   },
   "source": [
    "## Predictive Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be46484",
   "metadata": {},
   "source": [
    "Now that we have our dataset augmented with centrality measures, we can use it to train a predictive model. Let's use a simple logistic regression model from scikit-learn as an example.\n",
    "\n",
    "Firstly, you should split your dataset into training and testing sets. This is a common practice in machine learning to evaluate how well your model can generalize to unseen data.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_node_attributes.drop('Default', axis=1), df_node_attributes['Default'], test_size=0.2, random_state=42)\n",
    "```\n",
    "\n",
    "Now, let's train a logistic regression model on the training set:\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize a Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "Now that the model is trained, you can use it to predict the 'Default' status on the test set:\n",
    "\n",
    "```python\n",
    "# Predict the 'Default' status on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "```\n",
    "\n",
    "Finally, we can evaluate the performance of the model by computing metrics like accuracy, precision, recall and the F1 score:\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Compute evaluation metrics\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('Precision: ', precision_score(y_test, y_pred))\n",
    "print('Recall: ', recall_score(y_test, y_pred))\n",
    "print('F1 score: ', f1_score(y_test, y_pred))\n",
    "```\n",
    "\n",
    "This will give you a basic understanding of the performance of your model. Note that logistic regression is a relatively simple model, and depending on your dataset, you might achieve better performance with more complex models, such as Random Forests, Gradient Boosting Machines or Neural Networks. Also, always consider performing model validation (e.g., k-fold cross-validation) and hyperparameter tuning for a more reliable and better performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32331d5a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.480Z"
    }
   },
   "outputs": [],
   "source": [
    "df_node_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152e8b81",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.481Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_node_attributes.drop('attr_dict.Default', axis=1),\n",
    "    df_node_attributes['attr_dict.Default'],\n",
    "    test_size=0.2,\n",
    "    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44aa3333",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.482Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a55abc",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.483Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize a Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27441fe1",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.484Z"
    }
   },
   "outputs": [],
   "source": [
    "# Predict the 'Default' status on the test set\n",
    "y_pred_train = model.predict(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e73b978",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.485Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Compute evaluation metrics\n",
    "print('Accuracy Train: ', accuracy_score(y_train, y_pred_train))\n",
    "print('Precision Train: ', precision_score(y_train, y_pred_train))\n",
    "print('Recall Train: ', recall_score(y_train, y_pred_train))\n",
    "print('F1 score Train: ', f1_score(y_train, y_pred_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa9e76c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.486Z"
    }
   },
   "outputs": [],
   "source": [
    "# Predict the 'Default' status on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac53de5b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.487Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import evaluation metrics from sklearn.metrics\n",
    "# Accuracy measures the proportion of correct predictions out of total predictions\n",
    "# Precision measures the proportion of true positive predictions out of total positive predictions\n",
    "# Recall (also known as sensitivity) measures the proportion of true positive predictions out of total actual positives\n",
    "# F1 score is the harmonic mean of precision and recall, a balanced measure when classes are imbalanced\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Compute and print accuracy of the model on test data\n",
    "# This tells us the ratio of correctly predicted observations to the total observations\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Compute and print precision of the model on test data\n",
    "# This tells us the ratio of correctly predicted positive observations to the total predicted positives\n",
    "print('Precision: ', precision_score(y_test, y_pred))\n",
    "\n",
    "# Compute and print recall of the model on test data\n",
    "# This tells us the ratio of correctly predicted positive observations to the all observations in actual class\n",
    "print('Recall: ', recall_score(y_test, y_pred))\n",
    "\n",
    "# Compute and print the F1 score of the model on test data\n",
    "# The F1 score is the weighted average of Precision and Recall, used when we want to seek a balance between Precision and Recall\n",
    "print('F1 score: ', f1_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51da5153",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.488Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get feature names and corresponding coefficients\n",
    "feature_names = X_train.columns\n",
    "coefficients = model.coef_[0]\n",
    "\n",
    "# Create a DataFrame for easy visualization\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': coefficients\n",
    "})\n",
    "\n",
    "# Calculate the absolute values of the coefficients as a separate column\n",
    "coef_df['AbsCoefficient'] = np.abs(coef_df['Coefficient'])\n",
    "\n",
    "# Sort by absolute coefficient value in descending order\n",
    "coef_df = coef_df.sort_values('AbsCoefficient', ascending=False)\n",
    "\n",
    "print(coef_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a97f9c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.489Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix: \\n', cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34189422",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.490Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "auc_roc = roc_auc_score(y_test, y_pred)\n",
    "print('AUC-ROC: ', auc_roc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8892e54",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.491Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print('Classification Report: \\n', report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284fc0b5",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.491Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "print('Cross Validation Score: ', np.mean(scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dc35bf",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.492Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "plot_confusion_matrix(model, X_test, y_test)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1957868e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.493Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_roc_curve\n",
    "\n",
    "plot_roc_curve(model, X_test, y_test)\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519491e7",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.494Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "\n",
    "plot_precision_recall_curve(model, X_test, y_test)\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c48083b",
   "metadata": {},
   "source": [
    "### Grid search and CV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01be5f6e",
   "metadata": {},
   "source": [
    "Implement more complex models and apply validation techniques using scikit-learn for a Random Forest and a Gradient Boosting model. \n",
    "\n",
    "To make things more robust, I'll include a simple hyperparameter tuning using GridSearchCV, and model validation using k-fold cross-validation:\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Initialize the models\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Create a list of models\n",
    "models = [rf, gb]\n",
    "\n",
    "# Define the grid of hyperparameters 'params'\n",
    "params = [\n",
    "    {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20, 30], 'min_samples_split': [2, 5, 10]},  # RandomForest\n",
    "    {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 0.2], 'max_depth': [3, 5, 7]}   # GradientBoosting\n",
    "]\n",
    "\n",
    "for model, param in zip(models, params):\n",
    "    # GridSearchCV\n",
    "    grid = GridSearchCV(estimator=model, param_grid=param, cv=5)  # 5-fold cross-validation\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    print(grid.best_params_)  # print the best set of parameters found by GridSearch\n",
    "\n",
    "    # Predict the 'Default' status on the test set\n",
    "    y_pred = grid.predict(X_test)\n",
    "\n",
    "    # Compute evaluation metrics\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "```\n",
    "\n",
    "Please note, running this code may take a while, because it tries all combinations of the provided hyperparameters. Also, keep in mind that GridSearchCV applies cross-validation for model validation.\n",
    "\n",
    "As for Neural Networks, it's a bit more involved and typically requires more tuning and computational resources. scikit-learn does offer a simple `MLPClassifier` for multilayer perceptron (MLP) networks, but for more complex architectures, you'll want to look into deep learning libraries like TensorFlow or PyTorch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6701185e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.495Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Initialize the models\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Create a list of models\n",
    "models = [rf, gb]\n",
    "\n",
    "# Define the grid of hyperparameters 'params'\n",
    "params = [\n",
    "    {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20, 30], 'min_samples_split': [2, 5, 10]},  # RandomForest\n",
    "    {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 0.2], 'max_depth': [3, 5, 7]}   # GradientBoosting\n",
    "]\n",
    "\n",
    "for model, param in zip(models, params):\n",
    "    # GridSearchCV\n",
    "    grid = GridSearchCV(estimator=model, param_grid=param, cv=5)  # 5-fold cross-validation\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    print(grid.best_params_)  # print the best set of parameters found by GridSearch\n",
    "\n",
    "    # Predict the 'Default' status on the test set\n",
    "    y_pred = grid.predict(X_test)\n",
    "\n",
    "    # Compute evaluation metrics\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e86bdb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T21:07:03.319342Z",
     "start_time": "2023-05-19T21:07:03.298465Z"
    }
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ef4d6b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.496Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Predict the 'Default' status on the test set\n",
    "y_pred = grid.best_estimator_.predict(X_test)\n",
    "\n",
    "# Compute evaluation metrics\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b42bc1",
   "metadata": {},
   "source": [
    "### Interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8988f706",
   "metadata": {},
   "source": [
    "Feature importances should be calculated based on the training data. This is because the training data is what the model learns from, and feature importances are a measure of how much each feature contributes to the model's predictions.\n",
    "\n",
    "After training your model, you would typically look at the feature importances to understand which features the model considers important. You can use this information to gain insights into your model and your data. For example, you may find that some features are not important and could be removed, or that some features are very important and perhaps you want to spend more time engineering related features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36613814",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.497Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Compute permutation feature importance\n",
    "result = permutation_importance(grid.best_estimator_, X_test, y_test, n_repeats=10, random_state=42)\n",
    "\n",
    "# Create a DataFrame to visualize importance scores\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': X_test.columns,\n",
    "    'Permutation Importance': result.importances_mean,\n",
    "    'Std': result.importances_std\n",
    "})\n",
    "\n",
    "# Sort by importance\n",
    "importance_df = importance_df.sort_values('Permutation Importance', ascending=False)\n",
    "\n",
    "print(importance_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059ecf6c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.498Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get feature importances\n",
    "importances = grid.best_estimator_.feature_importances_\n",
    "\n",
    "# Create a DataFrame\n",
    "importances_df = pd.DataFrame({'feature': X_train.columns, 'importance': importances})\n",
    "\n",
    "# Sort by importance\n",
    "importances_df = importances_df.sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot\n",
    "importances_df.plot.bar(x='feature', y='importance')\n",
    "plt.title('Feature Importance')\n",
    "plt.ylabel('Importance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8f12eb",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.499Z"
    }
   },
   "outputs": [],
   "source": [
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "# Create a lime explainer object\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values,\n",
    "                                                   feature_names=X_train.columns.values.tolist(), \n",
    "                                                   class_names=['Non-Default', 'Default'], \n",
    "                                                   verbose=True, \n",
    "                                                   mode='classification')\n",
    "\n",
    "# Pick the observation in the validation set for which explanation is required\n",
    "observation_1 = X_test.values[0]\n",
    "\n",
    "# Get the explanation for RandomForest\n",
    "exp = explainer.explain_instance(observation_1, grid.best_estimator_.predict_proba, num_features=5)\n",
    "\n",
    "exp.show_in_notebook(show_table=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972ecd3d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-23T12:44:05.500Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "install shap\n",
    "import shap\n",
    "\n",
    "# Create object that can calculate shap values\n",
    "explainer = shap.TreeExplainer(grid.best_estimator_)\n",
    "\n",
    "# calculate shap values\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# plot\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dfabe1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T21:13:41.886609Z",
     "start_time": "2023-05-19T21:13:31.100683Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d390ec3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596ea033",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f510f90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca25ea9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791a92c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffecd0f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c0bb29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2080a1c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a6752d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af224db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec270c59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb51c30b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7b65b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228e7572",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7db502",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06940fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1420ea1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373fdefa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7c64c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a01e8ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97aaaab8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b57d9c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25e0793",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d4f04b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "幻灯片",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "263.505432px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
