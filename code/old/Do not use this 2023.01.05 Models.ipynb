{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "863066ee",
   "metadata": {},
   "source": [
    "# Models\n",
    "This notebook focus on the h2o models. It reads preprocessed data from disk, and conduc furthe analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee18791b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h2o\n",
    "from h2o.estimators import H2OGeneralizedLinearEstimator, H2ORandomForestEstimator, H2ODeepLearningEstimator\n",
    "from h2o.grid.grid_search import H2OGridSearch\n",
    "\n",
    "# 初始化H2O\n",
    "h2o.init()\n",
    "\n",
    "# 加载数据\n",
    "data = h2o.import_file(\"your_data.csv\")\n",
    "\n",
    "# 将数据分为训练集和测试集\n",
    "train, test = data.split_frame(ratios=[0.8])\n",
    "\n",
    "# 定义x（特征）和y（目标）\n",
    "x = list(train.columns)\n",
    "y = \"target_column\"\n",
    "x.remove(y)\n",
    "\n",
    "# 定义模型和超参数网格的列表\n",
    "models_and_hyperparams = [\n",
    "    {\n",
    "        'model': H2OGeneralizedLinearEstimator,\n",
    "        'hyper_params': {\n",
    "            'alpha': [0.01, 0.1, 0.5, 0.9, 0.99],\n",
    "            'lambda': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'model': H2ORandomForestEstimator,\n",
    "        'hyper_params': {\n",
    "            'ntrees': [50, 100, 150],\n",
    "            'max_depth': [3, 5, 7]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'model': H2ODeepLearningEstimator,\n",
    "        'hyper_params': {\n",
    "            'hidden': [[50, 50], [100, 100], [200, 200]],\n",
    "            'epochs': [10, 50, 100]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# 为每个模型和超参数组合执行Grid Search\n",
    "for model_info in models_and_hyperparams:\n",
    "    model = model_info['model']\n",
    "    hyper_params = model_info['hyper_params']\n",
    "\n",
    "    # 初始化Grid Search\n",
    "    grid = H2OGridSearch(\n",
    "        model=model,\n",
    "        hyper_params=hyper_params,\n",
    "        search_criteria={\"strategy\": \"Cartesian\"}\n",
    "    )\n",
    "\n",
    "    # 训练多个模型\n",
    "    grid.train(x=x, y=y, training_frame=train, validation_frame=test)\n",
    "\n",
    "    # 获取Grid Search结果\n",
    "    grid_results = grid.get_grid(sort_by=\"mse\", decreasing=False)\n",
    "\n",
    "    # 打印结果\n",
    "    print(grid_results)\n",
    "\n",
    "    # 将模型保存到磁盘\n",
    "    for model_id in grid_results.model_ids:\n",
    "        model_to_save = h2o.get_model(model_id)\n",
    "        h2o.save_model(model=model_to_save, path=\"your_model_directory\", force=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f485a5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0660395f",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc65d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import webbrowser\n",
    "import pandas as pd\n",
    "import importlib\n",
    "import subprocess\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix, roc_curve, auc,\n",
    "                             precision_recall_curve, average_precision_score)\n",
    "import h2o\n",
    "from h2o.estimators import H2ORandomForestEstimator\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from h2o.estimators import H2OGeneralizedLinearEstimator\n",
    "\n",
    "import os\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f1452a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87130867",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44bb0cc8",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84394305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_classification_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate and print binary classification metrics: accuracy, confusion matrix, and classification report.\n",
    "    \n",
    "    :param y_true: A pandas DataFrame or Series containing the true target values.\n",
    "    :param y_pred: A pandas DataFrame or Series containing the predicted target values.\n",
    "    :return: A dictionary containing the calculated classification metrics.\n",
    "    \"\"\"\n",
    "    # Calculate classification metrics\n",
    "    metrics = {}\n",
    "    metrics['accuracy'] = accuracy_score(y_true, y_pred)\n",
    "    metrics['confusion_matrix'] = confusion_matrix(y_true, y_pred)\n",
    "    metrics['classification_report'] = classification_report(y_true, y_pred)\n",
    "    \n",
    "    # Print the classification metrics\n",
    "    print('Accuracy:', metrics['accuracy'])\n",
    "    print('Confusion Matrix:\\n', metrics['confusion_matrix'])\n",
    "    print('Classification Report:\\n', metrics['classification_report'])\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a62f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(y_true,\n",
    "                         y_pred,\n",
    "                         positive_class=1,\n",
    "                         roc_file='roc_plot.html',\n",
    "                         pr_file='pr_plot.html'):\n",
    "    \"\"\"\n",
    "    Evaluate classification metrics, plot ROC curve and Precision-Recall curve using Plotly, and save plots as HTML files.\n",
    "\n",
    "    :param y_true: A pandas DataFrame or Series containing the true target values.\n",
    "    :param y_pred: A pandas DataFrame or Series containing the predicted target values.\n",
    "    :param positive_class: The label of the positive class (default: 1).\n",
    "    :param roc_file: The name of the HTML file to save the ROC curve plot (default: 'roc_plot.html').\n",
    "    :param pr_file: The name of the HTML file to save the Precision-Recall curve plot (default: 'pr_plot.html').\n",
    "    :return: A dictionary containing various classification metrics.\n",
    "    \"\"\"\n",
    "    # Calculate ROC curve and AUC\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred, pos_label=positive_class)\n",
    "    metrics['auc'] = auc(fpr, tpr)\n",
    "\n",
    "    # Calculate Precision-Recall curve and average precision\n",
    "    precision, recall, _ = precision_recall_curve(y_true,\n",
    "                                                  y_pred,\n",
    "                                                  pos_label=positive_class)\n",
    "    metrics['average_precision'] = average_precision_score(y_true, y_pred)\n",
    "\n",
    "    # Create ROC curve plot\n",
    "    fig_roc = make_subplots(rows=1, cols=1, subplot_titles=('ROC Curve', ))\n",
    "    fig_roc.add_trace(go.Scatter(x=fpr,\n",
    "                                 y=tpr,\n",
    "                                 mode='lines',\n",
    "                                 name='ROC Curve',\n",
    "                                 line=dict(color='blue')),\n",
    "                      row=1,\n",
    "                      col=1)\n",
    "    fig_roc.add_trace(go.Scatter(x=[0, 1],\n",
    "                                 y=[0, 1],\n",
    "                                 mode='lines',\n",
    "                                 name='Random',\n",
    "                                 line=dict(color='black', dash='dash')),\n",
    "                      row=1,\n",
    "                      col=1)\n",
    "    fig_roc.update_layout(title=f'ROC Curve (AUC = {metrics[\"auc\"]:.4f})',\n",
    "                          xaxis_title='False Positive Rate',\n",
    "                          yaxis_title='True Positive Rate',\n",
    "                          showlegend=True,\n",
    "                          legend=dict(orientation='h',\n",
    "                                      yanchor='bottom',\n",
    "                                      xanchor='right',\n",
    "                                      y=1.02,\n",
    "                                      x=1))\n",
    "\n",
    "    # Save the ROC curve plot as an HTML file\n",
    "    pio.write_html(fig_roc, file=roc_file)\n",
    "\n",
    "    # Create Precision-Recall curve plot\n",
    "    fig_pr = make_subplots(rows=1,\n",
    "                           cols=1,\n",
    "                           subplot_titles=('Precision-Recall Curve', ))\n",
    "    fig_pr.add_trace(go.Scatter(x=recall,\n",
    "                                y=precision,\n",
    "                                mode='lines',\n",
    "                                name='Precision-Recall Curve',\n",
    "                                line=dict(color='blue')),\n",
    "                     row=1,\n",
    "                     col=1)\n",
    "    fig_pr.update_layout(\n",
    "        title=\n",
    "        f'Precision-Recall Curve (Avg. Precision = {metrics[\"average_precision\"]:.4f})',\n",
    "        xaxis_title='Recall',\n",
    "        yaxis_title='Precision',\n",
    "        showlegend=True,\n",
    "        legend=dict(orientation='h',\n",
    "                    yanchor='bottom',\n",
    "                    xanchor='right',\n",
    "                    y=1.02,\n",
    "                    x=1))\n",
    "\n",
    "    # Save the Precision-Recall curve plot as an HTML file\n",
    "    pio.write_html(fig_pr, file=pr_file)\n",
    "\n",
    "    # Print classification metrics nicely\n",
    "\n",
    "    print(f'AUC: {metrics[\"auc\"]:.4f}')\n",
    "    print(f'Average Precision: {metrics[\"average_precision\"]:.4f}')\n",
    "\n",
    "    # Open the HTML files in the default web browser\n",
    "    webbrowser.open(roc_file)\n",
    "    webbrowser.open(pr_file)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70546321",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_h2o_model_to_disk(model, directory, filename):\n",
    "    \"\"\"\n",
    "    Given an H2O estimator object, a directory name, and a file name,\n",
    "    saves the trained model to disk using the provided file name and directory.\n",
    "\n",
    "    Args:\n",
    "        model (H2OEstimator): The H2O estimator object to save.\n",
    "        directory (str): The name of the directory to save the model in.\n",
    "        filename (str): The name of the file to save the model in.\n",
    "\n",
    "    Returns:\n",
    "        str: The full path to the saved model.\n",
    "\n",
    "    Example:\n",
    "        >>> model = h2o.estimators.random_forest.H2ORandomForestEstimator(ntrees=50, max_depth=20, nfolds=10, seed=42)\n",
    "        >>> data = h2o.import_file('example.csv')\n",
    "        >>> predictors = ['a', 'b']\n",
    "        >>> response = 'c'\n",
    "        >>> model.train(x=predictors, y=response, training_frame=data)\n",
    "        >>> directory = 'models'\n",
    "        >>> filename = 'rf_model'\n",
    "        >>> saved_model_path = save_h2o_model_to_disk(model, directory, filename)\n",
    "        >>> loaded_model = h2o.load_model(saved_model_path)\n",
    "    \"\"\"\n",
    "    # navigate up one level from cwd\n",
    "    parent_dir = os.path.dirname(os.getcwd())\n",
    "\n",
    "    # navigate into directory\n",
    "    dir_path = os.path.join(parent_dir, directory)\n",
    "\n",
    "    # create directory if it does not exist\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "    # merge directory path with file name\n",
    "    file_path = os.path.join(dir_path, filename)\n",
    "\n",
    "    # save model to file\n",
    "    #model.save_model(file_path)\n",
    "    h2o.save_model(model, file_path)\n",
    "    return file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7486fdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_h2o_model_from_disk(directory, filename):\n",
    "    \"\"\"\n",
    "    Given a directory name and a file name, loads an H2O model from disk.\n",
    "\n",
    "    Args:\n",
    "        directory (str): The name of the directory where the model is saved.\n",
    "        filename (str): The name of the file where the model is saved.\n",
    "\n",
    "    Returns:\n",
    "        H2OEstimator: The loaded H2O estimator object.\n",
    "\n",
    "    Example:\n",
    "        >>> directory = 'models'\n",
    "        >>> filename = 'rf_model'\n",
    "        >>> loaded_model = load_h2o_model_from_disk(directory, filename)\n",
    "        >>> predictions = loaded_model.predict(data)\n",
    "    \"\"\"\n",
    "    # Navigate up one level from cwd\n",
    "    parent_dir = os.path.dirname(os.getcwd())\n",
    "\n",
    "    # Navigate into directory\n",
    "    dir_path = os.path.join(parent_dir, directory)\n",
    "\n",
    "    # Merge directory path with file name\n",
    "    file_path = os.path.join(dir_path, filename)\n",
    "\n",
    "    # Load model from file\n",
    "    loaded_model = h2o.load_model(file_path)\n",
    "    return loaded_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04881104",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f00296d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6b7aae8",
   "metadata": {},
   "source": [
    "# Parameters 2: Parameters for model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f3c1b2",
   "metadata": {},
   "source": [
    "## Model directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fd1c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_directory = \"model\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "356.345123px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
